{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d999ba7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e819a605",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def closure(size_params,mu):\n",
    "    grad_est = []\n",
    "    \n",
    "    u = torch.normal(mean = torch.zeros(size_params),std = 1)\n",
    "    u = torch.div(u,torch.norm(u,\"fro\"))\n",
    "    \n",
    "    # initial evaluation\n",
    "    output = model(input)\n",
    "    loss_init = criterion.forward(output)\n",
    "    \n",
    "    # save the state of the model \n",
    "    model_init = dict(model.state_dict())\n",
    "    \n",
    "    start_ind = 0\n",
    "    for param_tensor in model.state_dict():\n",
    "        end_ind = start + model.state_dict()[param_tensor].view(-1).size()[0]\n",
    "        model.state_dict()[param_tensor].add_(u[start_ind:end_ind].view(model.state_dict()[param_tensor].size()), value = mu)\n",
    "    \n",
    "    # random evaluation\n",
    "    output2 = model(input)\n",
    "    loss_random = criterion.forward(output2)\n",
    "    \n",
    "    \n",
    "    # load initial state\n",
    "    model.load_state_dict(model_init)\n",
    "    \n",
    "    # compute the gradient\n",
    "    \n",
    "    grad_norm = size_params*(loss_random-loss_init)/mu\n",
    "    grad_est = []\n",
    "    \n",
    "    start_ind = 0\n",
    "    for param_tensor in model_init:\n",
    "        end_ind = start + model_init[param_tensor].view(-1).size()[0]\n",
    "        grad_est.apppend(grad_norm*u[start_ind:end_ind].view(model_init[param_tensor].size()))\n",
    "    \n",
    "    return grad_est\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42659f42",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ZO_AdaMM(Optimizer):\n",
    "    \n",
    "    def __init__(self,params,lr = 1e-03,betas = (0.9,0.999), mu = 1e-05, eps = 1e-12):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: (} - should be >= 0.0\". format (lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError (\"Invalid beta parameter: (} - should be in [0.0, 1.0[\". format (betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0l\". format (betas [1]))\n",
    "        if not 0.0 <= mu < 1.0:\n",
    "            raise ValueError(\"Invalid mu parameter: {} - should be in [0.0, 1.0l\". format (mu))\n",
    "            \n",
    "        defaults = dict(lr=lr, betas=betas, mu=mu, eps = eps)\n",
    "        super(ZO_AdaMM,self).__init__(params,defaults)\n",
    "        \n",
    "    def step(self, closure):\n",
    "        \n",
    "         for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            max_exp_avg_sqs = []\n",
    "            state_steps = []\n",
    "            beta1, beta2 = group['betas']\n",
    "            \n",
    "            size_params = 0\n",
    "            \n",
    "            for p in group['params']:\n",
    "                size_params += p.view(-1).size()[0]\n",
    "            \n",
    "            # closure return the approximation for the gradient, we have to add some \"option\" to this function \n",
    "            grad_est = closure(size_params,group[\"mu\"])\n",
    "            \n",
    "            i = 0\n",
    "            for p in group['params']:    \n",
    "                #grads.append(grad_est[i])\n",
    "                state = self.state[p]\n",
    "                # Lazy state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    #if group['amsgrad']:\n",
    "                    # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                    state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                exp_avgs.append(state['exp_avg'])\n",
    "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "\n",
    "                #if group['amsgrad']:\n",
    "                max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
    "\n",
    "                # update the steps for each param group update\n",
    "                state['step'] += 1\n",
    "                # record the step after step update\n",
    "                state_steps.append(state['step'])\n",
    "                    \n",
    "                \n",
    "                beta1, beta2 = group['betas']\n",
    "                state['exp_avg'].mul_(beta1).add_(grad_est[i],alpha = (1.0 - beta1))\n",
    "                state['exp_avg_sq'].mul_(beta2).addcmul_(grad_est[i], grad_est[i],value = (1.0 - beta2))\n",
    "                state['max_exp_avg_sq'] = torch.maximum(state['max_exp_avg_sq'],state['exp_avg_sq'])# vÃ©rifier max ou maximum\n",
    "                \n",
    "                p.data.addcdiv_(state['exp_avg'], state['exp_avg_sq'].sqrt().add_(group['eps']),value = (-group['lr']))\n",
    "                i +=1\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9db7933",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "901ed4cc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,12)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93585af3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 9, 3)\n",
    "        self.fc1 = nn.Linear(9 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5acd652",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "mnist_dataset_train = torchvision.datasets.MNIST('data/mnist/', download=True, train=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dataset_train, batch_size=4)\n",
    "\n",
    "mnist_dataset_test = torchvision.datasets.MNIST('data/mnist/', download=True, train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_dataset_train, batch_size=4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "nb_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44ffd7a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, nb_epochs, train_loader, test_loader):\n",
    "    # Heavily inspired from PyTorch tutorial\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            input, labels = data\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            outputs = model(input)\n",
    "\n",
    "            def closure(size_params, mu):\n",
    "                grad_est = {}\n",
    "                batch_size = labels.size(0)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    # initial evaluation\n",
    "                    output = outputs[i]\n",
    "                    print(labels)\n",
    "                    print(i)\n",
    "                    print(labels[i].size())\n",
    "                    print(output)\n",
    "                    loss = criterion(output, labels[i])\n",
    "\n",
    "                    running_loss = running_loss + loss.item()\n",
    "\n",
    "                    # save the state of the model\n",
    "                    model_init = dict(model.state_dict())\n",
    "\n",
    "                    #u = torch.normal(mean = torch.zeros(size_params),std = 1)\n",
    "                    u = 2*(torch.rand(size_params)-0.5)\n",
    "                    u.div_(torch.norm(u,\"fro\"))\n",
    "\n",
    "                    start_ind = 0\n",
    "                    for param_tensor in model.state_dict().values(): # CHECK if it is correctly ordered\n",
    "                        end_ind = start_ind + param_tensor.view(-1).size()[0]\n",
    "                        param_tensor.add_(u[start_ind:end_ind].view(param_tensor.size()), alpha = mu)\n",
    "                        start_ind = end_ind\n",
    "\n",
    "                    # random evaluation\n",
    "                    output2 = model(input)\n",
    "                    loss_random = criterion(output2,labels)\n",
    "\n",
    "\n",
    "                    # load initial state\n",
    "                    model.load_state_dict(model_init) # try to subtract the random vector to get back initial params\n",
    "\n",
    "                    # compute the gradient\n",
    "\n",
    "                    # when u is uniform random variable\n",
    "                    grad_norm = size_params*(loss_random-loss_init)/mu\n",
    "                    # when u is Gaussian random variable\n",
    "                    #grad_norm = (loss_random-loss_init)/mu\n",
    "\n",
    "                    start_ind = 0\n",
    "                    for layer_name, param_tensor in model_init.items():\n",
    "                        end_ind = start_ind + param_tensor.view(-1).size()[0]\n",
    "                        grad_est[layer_name] = grad_est.get(layer_name, 0) + grad_norm*u[start_ind:end_ind].view(param_tensor.size())\n",
    "                        start_ind = end_ind\n",
    "\n",
    "                return [v / batch_size for v in grad_est.values()]\n",
    "            \n",
    "            optimizer.step(closure)\n",
    "\n",
    "            if i % 2000 == 1999:\n",
    "                train_losses.append(running_loss / 2000)\n",
    "                print(f'epoch : {e + 1}/{nb_epochs} | train loss : {train_losses[-1]:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct_preds = 0\n",
    "            total_preds = 0\n",
    "\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                predictions = torch.argmax(outputs, 1)\n",
    "                total_preds += labels.size(0)\n",
    "                correct_preds += (predictions == labels).sum().item()\n",
    "\n",
    "            test_accuracies.append(correct_preds / total_preds)\n",
    "\n",
    "\n",
    "    return train_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f419aba2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "893e615e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 4, 1])\n",
      "0\n",
      "torch.Size([])\n",
      "tensor([-0.0755,  0.0048, -0.0526,  0.0742, -0.0931, -0.0501,  0.0847,  0.0299,\n",
      "        -0.0755,  0.0702])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-02037c9ceddd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0moptimizer_pt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mZO_AdaMM\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-4\u001B[0m\u001B[0;34m,\u001B[0m  \u001B[0mbetas\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.9\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.999\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmu\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;36m1e-03\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m     \u001B[0mtrain_losses_pt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_acc_pt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer_pt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnb_epochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-24-244478c64bef>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, optimizer, criterion, nb_epochs, train_loader, test_loader)\u001B[0m\n\u001B[1;32m     65\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mv\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mgrad_est\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclosure\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     68\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;36m2000\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1999\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/optim/optimizer.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m                 \u001B[0mprofile_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Optimizer.step#{}.step\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecord_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprofile_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m                     \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-8-720fbc33b5f7>\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m             \u001B[0;31m# closure return the approximation for the gradient, we have to add some \"option\" to this function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m             \u001B[0mgrad_est\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclosure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize_params\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mgroup\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"mu\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0mi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-24-244478c64bef>\u001B[0m in \u001B[0;36mclosure\u001B[0;34m(size_params, mu)\u001B[0m\n\u001B[1;32m     25\u001B[0m                     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m                     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m                     \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m                     \u001B[0mrunning_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrunning_loss\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1150\u001B[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001B[0m\u001B[1;32m   1151\u001B[0m                                \u001B[0mignore_index\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mignore_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduction\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1152\u001B[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   2844\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0msize_average\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mreduce\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2845\u001B[0m         \u001B[0mreduction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlegacy_get_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize_average\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2846\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcross_entropy_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_enum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreduction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel_smoothing\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2847\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2848\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "model = SmallModel()\n",
    "with torch.no_grad():\n",
    "    optimizer_pt = ZO_AdaMM(model.parameters(), lr=1e-4,  betas=(0.9, 0.999),mu =1e-03, eps=1e-10)\n",
    "\n",
    "    train_losses_pt, test_acc_pt = train(model, optimizer_pt, criterion, nb_epochs, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "932fc0bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, nb_epochs, train_loader, test_loader):\n",
    "    # Heavily inspired from PyTorch tutorial\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            #print(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            #print(loss)\n",
    "            #print('-----------------')\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 2000 == 1999:\n",
    "                train_losses.append(running_loss / 1000)\n",
    "                print(f'epoch : {e + 1}/{nb_epochs} | train loss : {train_losses[-1]:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct_preds = 0\n",
    "            total_preds = 0\n",
    "            print(model.state_dict())\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                predictions = torch.argmax(outputs, 1)\n",
    "                total_preds += labels.size(0)\n",
    "                correct_preds += (predictions == labels).sum().item()\n",
    "\n",
    "            test_accuracies.append(correct_preds / total_preds)\n",
    "\n",
    "    return train_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "a2c80e0b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/4 | train loss : 1.5687\n",
      "epoch : 1/4 | train loss : 0.5092\n",
      "epoch : 1/4 | train loss : 0.4239\n",
      "epoch : 1/4 | train loss : 0.3689\n",
      "epoch : 1/4 | train loss : 0.3468\n",
      "epoch : 1/4 | train loss : 0.2755\n",
      "epoch : 1/4 | train loss : 0.2991\n",
      "epoch : 1/4 | train loss : 0.2907\n",
      "epoch : 1/4 | train loss : 0.2372\n",
      "epoch : 1/4 | train loss : 0.1861\n",
      "epoch : 1/4 | train loss : 0.2051\n",
      "epoch : 1/4 | train loss : 0.1884\n",
      "epoch : 1/4 | train loss : 0.1939\n",
      "epoch : 1/4 | train loss : 0.2362\n",
      "epoch : 1/4 | train loss : 0.1997\n",
      "epoch : 1/4 | train loss : 0.2186\n",
      "epoch : 1/4 | train loss : 0.1821\n",
      "epoch : 1/4 | train loss : 0.1891\n",
      "epoch : 1/4 | train loss : 0.2003\n",
      "epoch : 1/4 | train loss : 0.1680\n",
      "epoch : 1/4 | train loss : 0.1970\n",
      "epoch : 1/4 | train loss : 0.2015\n",
      "epoch : 1/4 | train loss : 0.1618\n",
      "epoch : 1/4 | train loss : 0.2116\n",
      "epoch : 1/4 | train loss : 0.1958\n",
      "epoch : 1/4 | train loss : 0.1529\n",
      "epoch : 1/4 | train loss : 0.1914\n",
      "epoch : 1/4 | train loss : 0.1396\n",
      "epoch : 1/4 | train loss : 0.1295\n",
      "epoch : 1/4 | train loss : 0.1117\n",
      "OrderedDict([('conv1.weight', tensor([[[[-0.1346,  0.0637,  0.2727,  0.0680, -0.3561],\n",
      "          [ 0.1182,  0.1583, -0.0672, -0.1804, -0.1337],\n",
      "          [ 0.2037,  0.1219, -0.1615, -0.0878,  0.2931],\n",
      "          [ 0.2762,  0.1157,  0.1878,  0.3653,  0.0797],\n",
      "          [ 0.2777,  0.1109, -0.2486, -0.2879, -0.3701]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0578,  0.3892,  0.0985, -0.1544, -0.1185],\n",
      "          [ 0.3293,  0.0987,  0.0134, -0.2273, -0.0782],\n",
      "          [ 0.4182,  0.1637, -0.0859, -0.2687,  0.0115],\n",
      "          [ 0.2675,  0.2300, -0.1862, -0.2091, -0.1200],\n",
      "          [ 0.2062,  0.2438,  0.1195, -0.1369,  0.0494]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0385, -0.0138, -0.2398, -0.1926, -0.3670],\n",
      "          [-0.0115,  0.3682,  0.1411, -0.3565, -0.4007],\n",
      "          [ 0.0725,  0.4981,  0.5436,  0.1808,  0.0746],\n",
      "          [ 0.1550, -0.0160,  0.4054,  0.4225,  0.3460],\n",
      "          [-0.1684, -0.0500, -0.0249,  0.1373,  0.1671]]]])), ('conv1.bias', tensor([ 0.0635, -0.4064,  0.0247])), ('conv2.weight', tensor([[[[ 0.1141, -0.0905, -0.0463],\n",
      "          [-0.0208, -0.0782,  0.0504],\n",
      "          [-0.3364, -0.0917,  0.1342]],\n",
      "\n",
      "         [[-0.0094, -0.0028, -0.2337],\n",
      "          [-0.0906,  0.2960,  0.1018],\n",
      "          [-0.1304, -0.1582,  0.4084]],\n",
      "\n",
      "         [[ 0.1978, -0.1328, -0.6156],\n",
      "          [ 0.0631,  0.2965, -0.3868],\n",
      "          [-0.1987,  0.4931,  0.0707]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0116,  0.1173,  0.2182],\n",
      "          [ 0.1428,  0.2311,  0.1337],\n",
      "          [-0.3824, -0.3188, -0.1199]],\n",
      "\n",
      "         [[-0.0131, -0.0091,  0.1309],\n",
      "          [-0.0035, -0.0477,  0.0372],\n",
      "          [-0.1157, -0.5209, -0.4618]],\n",
      "\n",
      "         [[-0.2440,  0.1293,  0.1123],\n",
      "          [ 0.1636,  0.3919,  0.2143],\n",
      "          [-0.3138, -0.0992, -0.2530]]],\n",
      "\n",
      "\n",
      "        [[[-0.1199,  0.0212, -0.0750],\n",
      "          [-0.3501, -0.1079,  0.2286],\n",
      "          [-0.3185, -0.3075,  0.0852]],\n",
      "\n",
      "         [[-0.1508,  0.2642,  0.1489],\n",
      "          [-0.0343, -0.0965,  0.0731],\n",
      "          [-0.2918, -0.3586, -0.1297]],\n",
      "\n",
      "         [[ 0.1736,  0.2657, -0.2429],\n",
      "          [-0.0110,  0.3095,  0.0156],\n",
      "          [-0.4713,  0.2309,  0.1851]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1636,  0.0279,  0.3687],\n",
      "          [-0.2486, -0.0181, -0.0274],\n",
      "          [-0.1175, -0.0288, -0.3006]],\n",
      "\n",
      "         [[-0.0709, -0.0912, -0.1648],\n",
      "          [ 0.0749,  0.1493, -0.0892],\n",
      "          [ 0.1456, -0.2597, -0.3340]],\n",
      "\n",
      "         [[ 0.2451,  0.4082,  0.2743],\n",
      "          [-0.1105, -0.0214, -0.2586],\n",
      "          [-0.2191, -0.4713, -0.3554]]],\n",
      "\n",
      "\n",
      "        [[[-0.1782, -0.0871, -0.0696],\n",
      "          [-0.0208,  0.0436,  0.0163],\n",
      "          [ 0.0593,  0.1305, -0.0979]],\n",
      "\n",
      "         [[ 0.0083, -0.1913, -0.0370],\n",
      "          [-0.0864, -0.0783,  0.0027],\n",
      "          [-0.2250, -0.0166, -0.1223]],\n",
      "\n",
      "         [[-0.0917, -0.1513, -0.0268],\n",
      "          [-0.1755, -0.1482, -0.0163],\n",
      "          [-0.0708,  0.0084, -0.0776]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0518,  0.1922, -0.0888],\n",
      "          [ 0.2255, -0.0162, -0.4543],\n",
      "          [ 0.2304, -0.3409, -0.2161]],\n",
      "\n",
      "         [[ 0.2009,  0.2153, -0.1095],\n",
      "          [ 0.1956,  0.3426, -0.1752],\n",
      "          [ 0.3702, -0.2539,  0.0670]],\n",
      "\n",
      "         [[-0.0011, -0.1605,  0.0764],\n",
      "          [-0.0237, -0.3055,  0.0984],\n",
      "          [-0.0373, -0.1903,  0.0466]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0596, -0.1434,  0.0771],\n",
      "          [ 0.1531,  0.1254,  0.1738],\n",
      "          [-0.1320,  0.0138, -0.2244]],\n",
      "\n",
      "         [[ 0.0033, -0.1516,  0.0552],\n",
      "          [ 0.0156,  0.0342, -0.0666],\n",
      "          [-0.1583, -0.2219, -0.4617]],\n",
      "\n",
      "         [[-0.0283, -0.1832, -0.3576],\n",
      "          [ 0.2476,  0.3111,  0.3338],\n",
      "          [-0.1302, -0.0406,  0.1251]]],\n",
      "\n",
      "\n",
      "        [[[-0.3265, -0.0465, -0.1164],\n",
      "          [-0.2034, -0.0303, -0.1374],\n",
      "          [ 0.2305,  0.3050, -0.0444]],\n",
      "\n",
      "         [[-0.4148, -0.1147,  0.1542],\n",
      "          [-0.2752, -0.1623, -0.1307],\n",
      "          [-0.2743, -0.4176, -0.2043]],\n",
      "\n",
      "         [[-0.4342, -0.4090, -0.1514],\n",
      "          [ 0.0907, -0.2459, -0.3291],\n",
      "          [ 0.2922,  0.3728,  0.4219]]],\n",
      "\n",
      "\n",
      "        [[[-0.4591, -0.1968,  0.1879],\n",
      "          [-0.5505,  0.0310,  0.1442],\n",
      "          [-0.5778, -0.0148,  0.3527]],\n",
      "\n",
      "         [[-0.2514, -0.2273, -0.0035],\n",
      "          [-0.0488, -0.1237,  0.2595],\n",
      "          [-0.4253,  0.2913,  0.4011]],\n",
      "\n",
      "         [[-0.0629, -0.1003,  0.0299],\n",
      "          [-0.1733, -0.0667,  0.1891],\n",
      "          [-0.1583,  0.1411,  0.0193]]]])), ('conv2.bias', tensor([-0.4635, -0.0547, -0.5917,  0.0882, -0.0441, -0.1829, -0.3649, -0.0101,\n",
      "        -0.1250])), ('fc1.weight', tensor([[ 0.0182, -0.0663, -0.0752,  ..., -0.0096, -0.0775, -0.0269],\n",
      "        [-0.0489,  0.0528,  0.0521,  ..., -0.0657,  0.0203,  0.0274],\n",
      "        [ 0.0052, -0.0229,  0.0158,  ..., -0.0833, -0.0214, -0.0932],\n",
      "        ...,\n",
      "        [-0.0717, -0.1327, -0.0260,  ...,  0.1051, -0.0304,  0.1158],\n",
      "        [ 0.2084,  0.3527, -0.0169,  ..., -0.0007, -0.1297, -0.0352],\n",
      "        [ 0.0142, -0.0277,  0.1308,  ...,  0.1348,  0.0382,  0.0730]])), ('fc1.bias', tensor([-0.0517, -0.0487,  0.0113, -0.0314, -0.0534,  0.0226, -0.0215, -0.0313,\n",
      "         0.0083, -0.0989,  0.0383, -0.0676, -0.0472, -0.0446,  0.0200, -0.0526,\n",
      "        -0.0061, -0.0612, -0.0700,  0.0241, -0.0400, -0.0216,  0.0327, -0.0473,\n",
      "        -0.0351, -0.0518, -0.0753, -0.0066,  0.0263, -0.0567, -0.0559,  0.0489,\n",
      "         0.0156, -0.0820,  0.0459,  0.0160,  0.0178,  0.0631,  0.0423,  0.0170,\n",
      "        -0.0875, -0.0198, -0.0592,  0.0332, -0.0285, -0.1116,  0.1178, -0.0706,\n",
      "        -0.0621, -0.0629, -0.0734, -0.0582, -0.0338,  0.0210,  0.1653,  0.0360,\n",
      "         0.0053, -0.0230, -0.0674,  0.0187,  0.0029, -0.1372,  0.0715, -0.0163,\n",
      "        -0.0761, -0.1554,  0.0061, -0.0033, -0.1349,  0.0172,  0.1134, -0.0655,\n",
      "        -0.1024,  0.0136,  0.0403, -0.1532, -0.1270,  0.0749, -0.0061,  0.0053,\n",
      "         0.0544, -0.0221,  0.0745,  0.0089, -0.0760, -0.0837, -0.0043,  0.0110,\n",
      "         0.0762, -0.0299,  0.0623, -0.0833,  0.0038,  0.0325, -0.0993,  0.0012,\n",
      "        -0.1181,  0.0227,  0.0116, -0.0189, -0.0285, -0.0590,  0.1035, -0.0195,\n",
      "        -0.0272, -0.1173,  0.0503, -0.0613, -0.0033, -0.0186, -0.0565,  0.0167,\n",
      "        -0.0106, -0.0101, -0.0123, -0.0265, -0.0101,  0.1023, -0.0142,  0.0759])), ('fc2.weight', tensor([[ 4.0366e-02, -5.6626e-02, -5.8813e-02,  ..., -2.9725e-01,\n",
      "         -4.9703e-02, -5.6001e-02],\n",
      "        [ 4.3927e-02, -5.4157e-02,  5.5850e-03,  ...,  1.8835e-01,\n",
      "          1.9059e-02,  2.1349e-01],\n",
      "        [-4.1018e-02, -3.6765e-04,  8.6285e-02,  ..., -6.4376e-02,\n",
      "          3.1721e-02, -1.3948e-05],\n",
      "        ...,\n",
      "        [ 8.1418e-02, -1.2931e-03,  5.4890e-02,  ..., -1.8600e-01,\n",
      "         -3.7708e-02, -1.6659e-02],\n",
      "        [ 3.1362e-02, -2.0249e-02,  3.2312e-02,  ...,  7.4210e-02,\n",
      "          3.1195e-01, -1.6242e-02],\n",
      "        [-4.9548e-02, -7.4413e-02, -1.5638e-03,  ..., -1.8045e-01,\n",
      "         -8.4240e-02,  2.6819e-02]])), ('fc2.bias', tensor([ 0.0133, -0.0280, -0.0047,  0.0580,  0.2470,  0.0636, -0.0318, -0.0180,\n",
      "         0.0865,  0.1467,  0.1385,  0.2499, -0.0554,  0.1466,  0.1775,  0.1028,\n",
      "        -0.1071, -0.1315,  0.0322, -0.0362, -0.0666, -0.0878, -0.0770,  0.1841,\n",
      "         0.0629, -0.0091,  0.0689, -0.0635,  0.1161, -0.0393,  0.0685,  0.0500,\n",
      "         0.0268, -0.0111, -0.0251, -0.1339,  0.0302,  0.0517, -0.0709,  0.0275,\n",
      "         0.0236, -0.0729,  0.0114, -0.0028, -0.0173, -0.0931,  0.0426,  0.0173,\n",
      "         0.1128,  0.0638, -0.0819,  0.2165, -0.0386,  0.0604, -0.1287, -0.0613,\n",
      "         0.1645, -0.1549,  0.1345, -0.0754, -0.0563, -0.0522,  0.0074, -0.0756,\n",
      "         0.1337,  0.1075, -0.0109, -0.0808,  0.0153,  0.0997, -0.0885,  0.0803,\n",
      "         0.1178,  0.0879,  0.0515, -0.0234, -0.1244,  0.0065, -0.1255,  0.0561,\n",
      "        -0.1364, -0.1433,  0.0859,  0.0221])), ('fc3.weight', tensor([[ 1.4257e-01, -1.9748e-01,  1.0445e-01, -1.5623e-01, -2.2682e-01,\n",
      "         -1.0084e-01, -2.3685e-01, -4.0530e-02, -8.4713e-02,  9.6909e-02,\n",
      "          1.7282e-01,  1.1485e-01, -2.5998e-01, -1.5310e-01, -2.3748e-01,\n",
      "          1.1780e-01, -3.2509e-02, -1.1527e-01, -2.5819e-01, -2.1604e-01,\n",
      "          8.1494e-02,  1.1670e-01,  1.1681e-01, -8.9858e-02,  1.7657e-01,\n",
      "          1.7477e-01, -2.1827e-01,  2.5936e-03,  5.9579e-02, -1.1601e-01,\n",
      "         -2.4478e-01, -1.7343e-01, -1.4491e-01, -1.7817e-01,  6.1245e-02,\n",
      "         -3.2040e-02, -8.8473e-02,  1.0329e-01, -8.2975e-02, -2.1324e-01,\n",
      "          1.0852e-01, -1.6235e-01, -3.9237e-01, -1.6909e-02, -5.4536e-02,\n",
      "         -6.0350e-02,  1.7020e-01, -8.1573e-02, -7.1961e-02,  4.0014e-03,\n",
      "          1.7580e-02, -1.0214e-02,  1.3766e-01, -4.9550e-02,  1.0750e-01,\n",
      "         -4.9738e-02, -2.3905e-02,  4.6702e-02, -2.3632e-03, -1.1124e-01,\n",
      "         -1.8737e-01, -6.8582e-02, -1.3783e-01, -9.0149e-02, -8.7353e-02,\n",
      "          1.3844e-01, -1.4212e-01, -1.3859e-01, -8.9579e-03, -2.7838e-01,\n",
      "          7.1159e-02, -1.0214e-01, -1.7170e-01, -8.7126e-02,  1.0310e-01,\n",
      "         -1.4062e-01, -2.2539e-02,  1.8894e-01, -3.8687e-02, -3.0967e-01,\n",
      "         -2.1159e-01,  4.1417e-02,  3.2454e-02, -1.3593e-01],\n",
      "        [-2.5862e-01,  1.6728e-01, -2.6149e-01,  5.7629e-02, -3.5843e-02,\n",
      "         -2.8050e-01, -4.1027e-01,  2.1231e-01, -3.1887e-01,  1.9059e-01,\n",
      "         -1.5111e-01, -3.1665e-01, -1.2833e-01, -5.1934e-02,  7.0176e-02,\n",
      "          9.4776e-02,  6.3147e-02,  1.0347e-01, -1.1235e-01, -2.7660e-01,\n",
      "         -4.1383e-02, -2.9935e-01, -1.2159e-01, -7.3892e-03, -8.2244e-02,\n",
      "         -1.4192e-01,  1.8904e-01,  6.6245e-02,  4.1236e-03, -2.2300e-01,\n",
      "          1.4589e-01, -2.3501e-01,  7.4718e-02, -2.8477e-01, -2.0664e-01,\n",
      "         -5.0095e-02, -8.5970e-02,  1.5420e-01, -4.0852e-01,  2.4546e-02,\n",
      "         -3.1768e-01, -1.9667e-01,  4.5855e-02, -2.9848e-01,  2.7067e-01,\n",
      "         -1.3394e-01, -1.9803e-01,  1.7935e-01, -3.6681e-02,  4.5744e-02,\n",
      "          9.2475e-02, -1.8864e-01, -1.8034e-01,  1.6626e-01, -1.2954e-01,\n",
      "          9.6535e-02,  6.7051e-02, -3.8483e-01, -6.8572e-02, -8.6949e-02,\n",
      "          4.4373e-02, -4.3081e-02, -4.2497e-03, -1.0187e-01,  1.5619e-02,\n",
      "          8.6652e-02, -2.6143e-02, -3.0677e-01, -2.3891e-01, -4.2044e-02,\n",
      "          1.4520e-02,  2.0338e-02, -2.5702e-01,  4.4785e-03, -1.3257e-02,\n",
      "         -5.9115e-02,  8.1391e-02, -2.1465e-02, -4.7373e-02,  1.1710e-01,\n",
      "         -7.6436e-03, -4.5447e-01, -1.7163e-01,  1.7543e-02],\n",
      "        [-9.5599e-04, -1.3898e-01,  4.9839e-02, -2.3324e-01,  1.4405e-01,\n",
      "         -1.6279e-01, -5.5112e-02, -8.9735e-02, -2.1590e-01, -1.1270e-01,\n",
      "          1.2881e-01,  5.9467e-02, -1.6082e-01,  4.0301e-02,  7.4964e-02,\n",
      "         -1.7497e-01,  8.8597e-02, -9.4352e-02, -6.2397e-02, -1.8315e-01,\n",
      "          3.0607e-02, -7.3678e-02, -3.7613e-01,  6.1146e-02,  1.0586e-01,\n",
      "         -3.2265e-01, -2.6972e-01, -8.7893e-03, -2.8434e-01, -1.5792e-01,\n",
      "          6.1182e-02, -5.1857e-02,  9.2774e-03,  5.8044e-02, -4.8756e-02,\n",
      "         -9.1271e-02,  1.3842e-01,  3.3814e-02, -1.4575e-02, -3.1054e-01,\n",
      "          4.7062e-02, -8.6545e-02, -3.9696e-01, -2.2553e-01, -1.4806e-01,\n",
      "         -1.4704e-01, -2.7897e-01,  2.2860e-01, -2.4221e-01,  1.0288e-01,\n",
      "          1.1452e-01,  2.0414e-02, -2.2700e-02, -9.2761e-02, -3.7707e-02,\n",
      "          1.0310e-01, -2.8753e-01, -7.7763e-02, -3.0718e-02,  1.2321e-01,\n",
      "         -3.8904e-02, -1.2058e-01,  4.1852e-02, -3.9332e-02, -6.2317e-02,\n",
      "          1.5807e-01,  1.9658e-01,  1.1574e-01,  4.5360e-02,  2.0645e-01,\n",
      "          5.4606e-02, -2.6856e-01, -4.0975e-02,  1.9000e-03, -1.4688e-02,\n",
      "         -7.3712e-02,  6.7471e-02, -2.8535e-01, -1.0032e-02,  5.4604e-02,\n",
      "         -4.1611e-02, -5.3395e-03,  3.0293e-02, -2.0697e-02],\n",
      "        [-5.0632e-02, -1.2798e-01, -1.0765e-01, -1.4738e-01,  1.3594e-01,\n",
      "          1.8996e-01, -1.2939e-02,  9.5332e-02, -3.0754e-01, -1.0780e-01,\n",
      "         -4.4256e-02,  1.6278e-02, -3.8635e-01,  4.7045e-02, -1.0897e-01,\n",
      "         -4.8208e-02, -1.0504e-01, -1.0251e-01,  7.5052e-02,  2.3085e-02,\n",
      "         -2.3148e-01,  1.8560e-02, -1.6430e-01, -7.1355e-03, -2.4332e-01,\n",
      "         -1.0242e-01, -2.6588e-01,  1.8474e-01, -1.4628e-01, -7.8356e-02,\n",
      "         -7.8136e-02, -2.1204e-01, -9.7765e-02, -2.0927e-01, -3.4804e-01,\n",
      "         -1.6313e-01,  1.3674e-01,  6.5236e-02, -2.8826e-01, -2.7239e-03,\n",
      "         -1.4152e-01,  8.9943e-02,  1.5980e-01, -1.7885e-01,  5.1540e-02,\n",
      "         -3.3510e-02, -1.2559e-01, -3.4347e-01, -2.0379e-01, -5.7294e-02,\n",
      "         -1.4675e-01,  1.2363e-01, -1.2362e-01, -1.7364e-01, -1.1318e-01,\n",
      "         -1.6740e-01, -2.8792e-01, -1.3732e-01, -5.9575e-02, -1.6920e-01,\n",
      "         -2.4272e-01, -2.0818e-01, -1.0903e-01, -1.8123e-02,  2.2724e-01,\n",
      "         -9.5351e-02, -1.4920e-01, -2.0146e-01, -8.0418e-02,  2.6103e-03,\n",
      "         -1.7246e-01,  1.5662e-01, -1.0013e-01, -1.4538e-01,  7.3730e-02,\n",
      "         -7.0502e-02,  5.9101e-02,  7.3467e-02, -5.7074e-02,  1.0554e-01,\n",
      "         -6.8919e-02, -2.3971e-01, -1.1159e-01, -9.2876e-02],\n",
      "        [ 2.9975e-02,  9.8016e-02, -7.9773e-02,  1.5589e-01, -2.6629e-01,\n",
      "         -3.5363e-02,  5.4088e-02, -2.5840e-01,  5.8492e-03,  3.0986e-02,\n",
      "          4.2956e-02, -5.6975e-02,  1.6530e-01,  2.6066e-02, -4.3543e-02,\n",
      "         -9.1797e-03, -3.6581e-02, -9.1926e-02,  7.0389e-02,  1.1836e-01,\n",
      "          3.5609e-03, -2.6466e-01, -2.9853e-01, -3.9745e-01, -6.3802e-02,\n",
      "         -1.9149e-01,  1.6656e-01, -4.1606e-02, -1.2179e-01,  1.8572e-01,\n",
      "         -1.4408e-01,  1.2951e-01, -1.2013e-01,  1.3312e-01, -1.5050e-01,\n",
      "          9.1193e-02, -1.2129e-01,  4.5624e-02, -3.6939e-02, -2.1700e-01,\n",
      "         -1.5836e-01, -2.8161e-01,  1.9409e-02,  7.0390e-02, -2.7676e-01,\n",
      "          7.9734e-03,  5.7851e-02,  2.3841e-01, -2.7865e-02, -4.8545e-02,\n",
      "          2.9356e-02, -2.3993e-01, -7.0004e-02, -2.7175e-02, -9.1652e-02,\n",
      "         -1.8772e-01,  5.6199e-02, -1.3174e-01, -1.9843e-01, -8.5580e-02,\n",
      "          1.0623e-02, -4.5123e-02, -7.3735e-02, -5.2836e-02, -2.9610e-02,\n",
      "          4.1697e-02,  1.2056e-03,  3.8478e-03, -1.5134e-01, -2.2818e-01,\n",
      "         -1.3978e-02, -2.4044e-02,  8.1689e-02,  3.5770e-02, -5.5449e-02,\n",
      "         -2.2669e-01, -5.8073e-02, -1.6560e-03, -3.7099e-02,  1.1794e-01,\n",
      "          8.0956e-02,  8.6539e-03, -1.9354e-01, -2.5597e-01],\n",
      "        [-1.5751e-01,  2.7849e-02,  6.5443e-03, -1.6299e-01, -1.6375e-01,\n",
      "          8.9303e-02, -4.5080e-03,  2.0267e-01,  1.7164e-04, -1.4376e-01,\n",
      "         -1.7184e-02, -1.6465e-01, -2.4754e-01, -2.2147e-02, -2.9121e-01,\n",
      "         -1.7607e-01, -5.2911e-02, -2.1002e-01, -2.3082e-01,  1.0732e-01,\n",
      "          2.1882e-02,  9.4166e-02, -1.2145e-01, -2.5811e-01, -1.9565e-01,\n",
      "         -6.2483e-02, -5.5657e-02, -2.6337e-02, -1.3074e-01, -2.3000e-01,\n",
      "         -4.5182e-02,  5.6229e-02, -4.9122e-02,  2.1211e-03, -2.5393e-01,\n",
      "         -8.2189e-02,  5.5222e-02, -8.3622e-02,  1.3783e-02,  1.3623e-01,\n",
      "         -1.1294e-01,  1.3458e-01,  7.2479e-02,  7.5094e-02,  1.6116e-01,\n",
      "         -8.8119e-02, -2.3205e-01, -5.8468e-02, -2.4108e-02, -2.3047e-01,\n",
      "         -5.7552e-02,  1.4529e-01, -1.1832e-01, -2.1545e-02, -4.8142e-01,\n",
      "         -2.3994e-01, -2.0090e-02, -1.2958e-01, -2.0570e-02, -6.4865e-02,\n",
      "         -1.9516e-01,  2.6978e-01, -1.9369e-01, -1.3588e-01, -2.1966e-01,\n",
      "         -2.1938e-01, -3.0496e-02, -1.9786e-01, -2.0364e-02, -2.5942e-02,\n",
      "          3.5214e-02, -8.7402e-02, -2.0049e-01, -9.5276e-03,  2.0195e-02,\n",
      "          5.2323e-02,  4.6425e-04,  1.2722e-01, -6.4296e-02, -2.0674e-01,\n",
      "          1.2453e-01, -2.9106e-02,  9.6322e-02,  2.6877e-02],\n",
      "        [ 1.0132e-01, -1.1554e-02,  7.0526e-02, -1.9780e-01, -1.1651e-01,\n",
      "         -2.6880e-01,  8.0900e-02, -2.4190e-02,  5.6329e-02,  7.0004e-02,\n",
      "         -1.6315e-01, -8.2632e-03, -1.6465e-01, -4.4305e-01, -4.9869e-01,\n",
      "         -2.9173e-02,  5.4675e-02, -1.5235e-01,  1.8198e-01,  3.5657e-02,\n",
      "          1.4794e-01, -2.3642e-01, -9.4740e-02, -3.6731e-01, -2.8112e-01,\n",
      "          1.3727e-01,  3.5724e-02, -1.6900e-01,  1.4809e-01, -1.6162e-01,\n",
      "          5.6559e-02,  1.0193e-01, -2.0409e-01,  9.0745e-02,  3.2689e-02,\n",
      "         -1.4971e-01,  2.0458e-02,  4.2754e-02, -6.5245e-02,  4.1398e-02,\n",
      "         -7.9999e-02, -2.1377e-01, -1.2533e-01,  8.6431e-02,  8.1382e-02,\n",
      "         -9.9719e-03,  4.9250e-03, -1.5736e-02, -4.3092e-02, -7.1476e-02,\n",
      "          7.8012e-02,  2.8403e-02,  6.4815e-02, -8.4579e-02, -2.1431e-01,\n",
      "         -1.5105e-01, -2.7426e-01, -1.0864e-01, -6.6318e-03, -4.4120e-02,\n",
      "         -1.6459e-02,  4.1374e-02, -2.6515e-01, -6.6833e-02, -2.7633e-01,\n",
      "         -1.6351e-01,  3.7994e-02, -7.7480e-02,  1.4882e-01,  6.2697e-02,\n",
      "          7.0104e-02, -6.4644e-03, -2.6060e-01, -1.2131e-01, -7.5627e-02,\n",
      "          1.4553e-01,  5.3098e-02,  1.0110e-01, -7.9528e-02, -3.4646e-01,\n",
      "         -7.7725e-03, -2.3539e-01, -1.6769e-01,  1.4436e-01],\n",
      "        [-5.0528e-02, -1.1385e-01, -1.2777e-01, -1.0305e-01, -5.4600e-02,\n",
      "         -7.3686e-02,  2.0645e-01, -6.9455e-02, -5.7859e-02, -1.0885e-01,\n",
      "         -1.6941e-01, -3.3263e-01,  5.1609e-02,  2.7912e-02,  1.0078e-01,\n",
      "          1.5289e-02,  6.6862e-02,  8.0904e-03,  6.4938e-02, -2.3048e-01,\n",
      "          1.1346e-01, -7.8344e-04, -1.0353e-01,  6.6473e-02,  7.9600e-02,\n",
      "         -3.5719e-01,  1.0943e-01,  8.3352e-02, -6.0321e-01, -1.3861e-01,\n",
      "         -8.3984e-02, -2.0034e-01, -1.4882e-02, -8.8859e-02, -7.8405e-02,\n",
      "         -2.1733e-01, -1.2625e-01,  1.9136e-01, -2.5696e-01, -5.7972e-01,\n",
      "         -4.4540e-03,  1.2737e-01, -8.5136e-02, -1.8577e-01, -1.8480e-01,\n",
      "          1.9547e-02,  5.2705e-03, -1.6144e-01, -1.7071e-01, -2.7024e-03,\n",
      "         -3.4191e-02, -3.3723e-01, -1.2797e-01, -2.3141e-01, -1.3473e-02,\n",
      "         -9.7151e-02,  4.2892e-02, -2.2550e-01, -3.1256e-01, -5.6526e-02,\n",
      "         -3.6555e-03,  1.8955e-01,  1.6882e-01,  2.3898e-02, -1.5258e-01,\n",
      "         -5.2837e-02,  3.3388e-02, -4.0242e-02, -3.1324e-01, -2.0038e-02,\n",
      "         -1.1419e-01, -4.6605e-02,  2.0961e-01, -1.4577e-01,  1.1543e-01,\n",
      "         -1.5125e-01,  2.7363e-02, -3.2659e-02, -8.1420e-02,  2.2915e-01,\n",
      "         -2.5774e-02, -1.3429e-01,  1.4460e-01, -2.4643e-01],\n",
      "        [-1.7951e-01, -3.8032e-02,  1.5641e-01, -1.0188e-01,  6.1956e-02,\n",
      "          5.0412e-02, -1.2306e-01, -1.2516e-02,  1.0151e-01,  7.5108e-02,\n",
      "         -1.8696e-01,  9.8637e-02, -3.2484e-02,  4.4514e-03,  9.8021e-02,\n",
      "          1.0370e-01,  1.2486e-02, -2.2123e-01,  1.3152e-01,  6.5162e-02,\n",
      "          1.1543e-01, -9.1950e-02, -3.2903e-02,  4.5365e-02, -1.1257e-01,\n",
      "         -1.7746e-01, -2.3594e-01, -7.0840e-02,  2.8629e-02, -1.9992e-01,\n",
      "          4.7174e-02,  1.1341e-01,  3.9569e-02, -1.5491e-02, -1.1387e-01,\n",
      "         -3.1202e-01, -1.0094e-01, -1.9355e-01, -4.3186e-01,  9.8309e-03,\n",
      "         -1.1402e-01, -1.6508e-01,  1.0627e-01, -5.1206e-02, -2.0720e-01,\n",
      "         -3.9615e-02, -1.7270e-01, -1.7022e-01,  2.6461e-01,  8.4308e-02,\n",
      "         -1.1453e-01,  1.4852e-01, -7.6773e-02, -2.2113e-01, -2.8055e-01,\n",
      "         -9.3139e-02,  3.2265e-02, -3.0390e-01,  7.9141e-02, -5.2693e-02,\n",
      "         -2.3137e-01, -1.7995e-01,  6.2329e-02, -8.0858e-02, -2.1473e-01,\n",
      "         -7.0084e-02, -3.5098e-02, -7.9952e-02, -2.1876e-01,  1.2794e-01,\n",
      "         -1.3217e-02,  4.0264e-02, -2.2030e-01,  6.5811e-02,  3.5244e-02,\n",
      "         -8.6471e-02,  8.5656e-02, -1.7729e-01,  5.8756e-02, -2.2629e-01,\n",
      "         -1.1137e-01, -1.8616e-01, -3.9357e-02, -2.0602e-02],\n",
      "        [-2.6789e-02, -2.2558e-02,  3.3190e-02,  7.4306e-02, -3.0967e-01,\n",
      "          2.2490e-01, -3.3449e-01, -5.4421e-02,  1.4141e-01, -3.7567e-02,\n",
      "          8.5319e-02, -1.8899e-01, -1.0497e-01,  5.0098e-02,  1.7506e-02,\n",
      "          9.9526e-02, -9.8020e-03, -1.5962e-01, -1.2551e-01,  1.7012e-01,\n",
      "         -1.1530e-01,  3.3781e-03, -1.3689e-01,  5.9978e-02,  6.0545e-02,\n",
      "          1.2805e-02,  3.6342e-02, -2.9379e-01, -4.1335e-01,  1.4042e-01,\n",
      "          1.9164e-02,  5.2186e-02, -6.0860e-02, -1.1789e-01, -3.1324e-01,\n",
      "         -1.5429e-01, -1.6653e-01, -2.2974e-01, -6.1528e-02, -2.0084e-01,\n",
      "         -2.8385e-01, -9.9908e-02,  8.9633e-02,  8.3106e-02, -2.7177e-02,\n",
      "         -1.4976e-01,  1.6399e-01, -2.2426e-01, -2.1509e-01, -2.4379e-01,\n",
      "         -2.5937e-01, -1.3002e-02, -7.9806e-02,  5.7280e-02, -1.0774e-01,\n",
      "         -1.9492e-01,  9.2180e-02, -4.0861e-01, -1.9048e-01,  6.1373e-02,\n",
      "         -1.1561e-01, -2.2686e-01, -2.1978e-01, -1.3721e-01,  1.4342e-01,\n",
      "         -1.1267e-01, -1.5366e-01,  1.7121e-01, -1.5772e-01, -1.8450e-01,\n",
      "         -6.0666e-02,  1.5129e-01,  2.1133e-01,  4.6059e-02,  1.0379e-01,\n",
      "         -7.4352e-02,  1.7600e-02,  1.0788e-01, -1.4606e-01,  2.5415e-02,\n",
      "         -2.1707e-01,  4.0107e-02, -2.1065e-01, -2.8371e-01]])), ('fc3.bias', tensor([ 0.1128, -0.0832, -0.0027, -0.0489, -0.1245,  0.0309, -0.1491, -0.1311,\n",
      "         0.1043, -0.0328]))])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_80477/3483209665.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0moptimizer_pt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moptim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msmall_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbetas\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.9\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.999\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-8\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mamsgrad\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mtrain_losses_pt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_acc_pt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msmall_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer_pt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnb_epochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_80477/2866243822.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, optimizer, criterion, nb_epochs, train_loader, test_loader)\u001B[0m\n\u001B[1;32m     34\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m                 \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m                 \u001B[0mpredictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_80477/196871199.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpool\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfc1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfc2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfc3\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1094\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1095\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1096\u001B[0;31m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1097\u001B[0m         \u001B[0mforward_call\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_tracing_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1098\u001B[0m         \u001B[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "small_model = SmallModel()\n",
    "\n",
    "optimizer_pt = optim.Adam(small_model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8, amsgrad=True)\n",
    "\n",
    "train_losses_pt, test_acc_pt = train(small_model, optimizer_pt, criterion, nb_epochs, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a205f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}