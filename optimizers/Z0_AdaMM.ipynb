{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d999ba7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e819a605",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def closure(size_params,mu):\n",
    "    grad_est = []\n",
    "    \n",
    "    u = torch.normal(mean = torch.zeros(size_params),std = 1)\n",
    "    u = torch.div(u,torch.norm(u,\"fro\"))\n",
    "    \n",
    "    # initial evaluation\n",
    "    output = model(input)\n",
    "    loss_init = criterion.forward(output)\n",
    "    \n",
    "    # save the state of the model \n",
    "    model_init = dict(model.state_dict())\n",
    "    \n",
    "    start_ind = 0\n",
    "    for param_tensor in model.state_dict():\n",
    "        end_ind = start + model.state_dict()[param_tensor].view(-1).size()[0]\n",
    "        model.state_dict()[param_tensor].add_(u[start_ind:end_ind].view(model.state_dict()[param_tensor].size()), value = mu)\n",
    "    \n",
    "    # random evaluation\n",
    "    output2 = model(input)\n",
    "    loss_random = criterion.forward(output2)\n",
    "    \n",
    "    \n",
    "    # load initial state\n",
    "    model.load_state_dict(model_init)\n",
    "    \n",
    "    # compute the gradient\n",
    "    \n",
    "    grad_norm = size_params*(loss_random-loss_init)/mu\n",
    "    grad_est = []\n",
    "    \n",
    "    start_ind = 0\n",
    "    for param_tensor in model_init:\n",
    "        end_ind = start + model_init[param_tensor].view(-1).size()[0]\n",
    "        grad_est.apppend(grad_norm*u[start_ind:end_ind].view(model_init[param_tensor].size()))\n",
    "    \n",
    "    return grad_est\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42659f42",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ZO_AdaMM(Optimizer):\n",
    "    \n",
    "    def __init__(self,params,lr = 1e-03,betas = (0.9,0.999), mu = 1e-05, eps = 1e-12):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: (} - should be >= 0.0\". format (lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError (\"Invalid beta parameter: (} - should be in [0.0, 1.0[\". format (betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0l\". format (betas [1]))\n",
    "        if not 0.0 <= mu < 1.0:\n",
    "            raise ValueError(\"Invalid mu parameter: {} - should be in [0.0, 1.0l\". format (mu))\n",
    "            \n",
    "        defaults = dict(lr=lr, betas=betas, mu=mu, eps = eps)\n",
    "        super(ZO_AdaMM,self).__init__(params,defaults)\n",
    "        \n",
    "    def step(self, closure):\n",
    "        \n",
    "         for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            max_exp_avg_sqs = []\n",
    "            state_steps = []\n",
    "            beta1, beta2 = group['betas']\n",
    "            \n",
    "            size_params = 0\n",
    "            \n",
    "            for p in group['params']:\n",
    "                size_params += p.view(-1).size()[0]\n",
    "            \n",
    "            # closure return the approximation for the gradient, we have to add some \"option\" to this function \n",
    "            grad_est = closure(size_params,group[\"mu\"])\n",
    "            \n",
    "            i = 0\n",
    "            for p in group['params']:    \n",
    "                #grads.append(grad_est[i])\n",
    "                state = self.state[p]\n",
    "                # Lazy state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    #if group['amsgrad']:\n",
    "                    # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                    state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                exp_avgs.append(state['exp_avg'])\n",
    "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "\n",
    "                #if group['amsgrad']:\n",
    "                max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
    "\n",
    "                # update the steps for each param group update\n",
    "                state['step'] += 1\n",
    "                # record the step after step update\n",
    "                state_steps.append(state['step'])\n",
    "                    \n",
    "                \n",
    "                beta1, beta2 = group['betas']\n",
    "                state['exp_avg'].mul_(beta1).add_(grad_est[i],alpha = (1.0 - beta1))\n",
    "                state['exp_avg_sq'].mul_(beta2).addcmul_(grad_est[i], grad_est[i],value = (1.0 - beta2))\n",
    "                state['max_exp_avg_sq'] = torch.maximum(state['max_exp_avg_sq'],state['exp_avg_sq'])# vÃ©rifier max ou maximum\n",
    "                \n",
    "                p.data.addcdiv_(state['exp_avg'], state['exp_avg_sq'].sqrt().add_(group['eps']),value = (-group['lr']))\n",
    "                i +=1\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9db7933",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901ed4cc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,12)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93585af3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 9, 3)\n",
    "        self.fc1 = nn.Linear(9 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a4e5c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 9, 3)\n",
    "        #self.fc1 = nn.Linear(9 * 5 * 5, 15)\n",
    "        #self.fc2 = nn.Linear(15, 13)\n",
    "        self.fc3 = nn.Linear(9 * 5 * 5, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5acd652",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9912422 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31b44a967232445a9ae54a83f3388ff3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/28881 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f63bcfc4c580465b83b2b349b45ad370"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1648877 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b9b8c9fceba45e1bab1aba18322cb13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4542 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36a83a3dee0a4ab6ad01a51a6ee8ca7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "mnist_dataset_train = torchvision.datasets.MNIST('data/mnist/', download=True, train=True, transform=transform,)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dataset_train, batch_size=100)\n",
    "\n",
    "mnist_dataset_test = torchvision.datasets.MNIST('data/mnist/', download=True, train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_dataset_train, batch_size=200)\n",
    "\n",
    "\"\"\"transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "mnist_dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dataset_train, batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_dataset_test, batch_size=100)\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss() #\n",
    "\n",
    "nb_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ffd7a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "' TRAIN WITH NON WORKING CLOSURE ON EFFICIENTNET (CRASHING FOR LONG TYPE MISMATCH)\\n\\ndef train(model, optimizer, criterion, nb_epochs, train_loader, test_loader):\\n    # Heavily inspired from PyTorch tutorial\\n    train_losses = []\\n    test_accuracies = []\\n\\n    #global running_loss\\n    running_loss = 0\\n    lr_init = optimizer_pt.param_groups[0][\\'lr\\']\\n    mu_init = optimizer_pt.param_groups[0][\\'mu\\']\\n\\n    for e in range(nb_epochs):\\n        ## change the learning rate alon iteration\\n        optimizer_pt.param_groups[0][\\'lr\\'] = max(lr_init/(np.sqrt(10)**(e/4.)),1e-5)\\n        optimizer_pt.param_groups[0][\\'mu\\'] = max(mu_init/(np.sqrt(10)**(e/4.)),1e-5)\\n        for i, data in enumerate(train_loader):\\n            input, labels = data\\n\\n            #optimizer.zero_grad()\\n\\n            ## initial evaluation\\n            outputs = model(input)\\n            loss_init = criterion(outputs, labels)\\n\\n            running_loss = running_loss + loss_init.item()\\n\\n            batch_size = labels.size(0)\\n\\n            def closure(size_params, mu):\\n                grad_est = []\\n\\n\\n\\n                ## Generate a random direction uniformly on the unit ball or with a gaussian distribution\\n                #u = torch.normal(mean = torch.zeros(size_params),std = 100)\\n                u = 2*(torch.rand(size_params)-0.5) # need small modif in order to be on the unit sphere\\n                u.div_(torch.norm(u,\"fro\"))\\n\\n                ## save the state of the model\\n                model_init = dict(model.state_dict())\\n                grad_norm = 0\\n\\n                ## we add to the inital parameters a random perturbation times \\\\mu\\n                start_ind = 0\\n                for param_tensor in model.state_dict().values():\\n                    end_ind = start_ind + param_tensor.view(-1).size()[0]\\n                    param_tensor.add_(u[start_ind:end_ind].view(param_tensor.size()), alpha = mu)\\n                    start_ind = end_ind\\n\\n                ## evaluation of the model and the with a random perturbation of the parameters\\n                output2 = model(input)\\n                loss_random = criterion(output2,labels)\\n\\n                ## compute the \"gradient norm\"\\n\\n                ## when u is uniform random variable\\n                grad_norm = size_params*(loss_random-loss_init)/mu\\n                ## when u is Gaussian random variable\\n                #grad_norm += (loss_random-loss_init)/mu\\n                #print(grad_norm,(loss_random-loss_init))\\n\\n                start_ind = 0\\n                for param_tensor in model_init.values():\\n                    end_ind = start_ind + param_tensor.view(-1).size()[0]\\n                    grad_est.append((grad_norm/batch_size)*u[start_ind:end_ind].view(param_tensor.size()))\\n                    start_ind = end_ind\\n\\n                ## reload initial state of the parameters\\n                model.load_state_dict(model_init) # try to subtract the random vector to get back initial params\\n\\n                return grad_est\\n\\n            optimizer.step(closure)\\n\\n            if i % 2000 == 1999:\\n                train_losses.append(running_loss / (batch_size*2000))\\n                print(f\\'epoch : {e + 1}/{nb_epochs} | train loss : {train_losses[-1]:.4f}\\')\\n                running_loss = 0.0\\n\\n        with torch.no_grad():\\n            correct_preds = 0\\n            total_preds = 0\\n\\n            for inputs, labels in test_loader:\\n                outputs = model(inputs)\\n\\n                predictions = torch.argmax(outputs, 1)\\n                total_preds += labels.size(0)\\n                correct_preds += (predictions == labels).sum().item()\\n\\n            test_accuracies.append(correct_preds / total_preds)\\n            print(f\\'epoch : {e + 1}/{nb_epochs} | test accuracies : {correct_preds / total_preds}\\')\\n\\n    return train_losses, test_accuracies\\n    '"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TRAIN WITH NON WORKING CLOSURE ON EFFICIENTNET (CRASHING FOR LONG TYPE MISMATCH)\n",
    "\n",
    "def train(model, optimizer, criterion, nb_epochs, train_loader, test_loader):\n",
    "    # Heavily inspired from PyTorch tutorial\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    #global running_loss\n",
    "    running_loss = 0\n",
    "    lr_init = optimizer_pt.param_groups[0]['lr']\n",
    "    mu_init = optimizer_pt.param_groups[0]['mu']\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        ## change the learning rate alon iteration\n",
    "        optimizer_pt.param_groups[0]['lr'] = max(lr_init/(np.sqrt(10)**(e/4.)),1e-5)\n",
    "        optimizer_pt.param_groups[0]['mu'] = max(mu_init/(np.sqrt(10)**(e/4.)),1e-5)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            input, labels = data\n",
    "\n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "            ## initial evaluation\n",
    "            outputs = model(input)\n",
    "            loss_init = criterion(outputs, labels)\n",
    "\n",
    "            running_loss = running_loss + loss_init.item()\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "\n",
    "            def closure(size_params, mu):\n",
    "                grad_est = []\n",
    "\n",
    "\n",
    "\n",
    "                ## Generate a random direction uniformly on the unit ball or with a gaussian distribution\n",
    "                #u = torch.normal(mean = torch.zeros(size_params),std = 100)\n",
    "                u = 2*(torch.rand(size_params)-0.5) # need small modif in order to be on the unit sphere\n",
    "                u.div_(torch.norm(u,\"fro\"))\n",
    "\n",
    "                ## save the state of the model\n",
    "                model_init = dict(model.state_dict())\n",
    "                grad_norm = 0\n",
    "\n",
    "                ## we add to the inital parameters a random perturbation times \\mu\n",
    "                start_ind = 0\n",
    "                for param_tensor in model.state_dict().values():\n",
    "                    end_ind = start_ind + param_tensor.view(-1).size()[0]\n",
    "                    param_tensor.add_(u[start_ind:end_ind].view(param_tensor.size()), alpha = mu)\n",
    "                    start_ind = end_ind\n",
    "\n",
    "                ## evaluation of the model and the with a random perturbation of the parameters\n",
    "                output2 = model(input)\n",
    "                loss_random = criterion(output2,labels)\n",
    "\n",
    "                ## compute the \"gradient norm\"\n",
    "\n",
    "                ## when u is uniform random variable\n",
    "                grad_norm = size_params*(loss_random-loss_init)/mu\n",
    "                ## when u is Gaussian random variable\n",
    "                #grad_norm += (loss_random-loss_init)/mu\n",
    "                #print(grad_norm,(loss_random-loss_init))\n",
    "\n",
    "                start_ind = 0\n",
    "                for param_tensor in model_init.values():\n",
    "                    end_ind = start_ind + param_tensor.view(-1).size()[0]\n",
    "                    grad_est.append((grad_norm/batch_size)*u[start_ind:end_ind].view(param_tensor.size()))\n",
    "                    start_ind = end_ind\n",
    "\n",
    "                ## reload initial state of the parameters\n",
    "                model.load_state_dict(model_init) # try to subtract the random vector to get back initial params\n",
    "\n",
    "                return grad_est\n",
    "\n",
    "            optimizer.step(closure)\n",
    "\n",
    "            if i % 2000 == 1999:\n",
    "                train_losses.append(running_loss / (batch_size*2000))\n",
    "                print(f'epoch : {e + 1}/{nb_epochs} | train loss : {train_losses[-1]:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct_preds = 0\n",
    "            total_preds = 0\n",
    "\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                predictions = torch.argmax(outputs, 1)\n",
    "                total_preds += labels.size(0)\n",
    "                correct_preds += (predictions == labels).sum().item()\n",
    "\n",
    "            test_accuracies.append(correct_preds / total_preds)\n",
    "            print(f'epoch : {e + 1}/{nb_epochs} | test accuracies : {correct_preds / total_preds}')\n",
    "\n",
    "    return train_losses, test_accuracies\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe0e6ae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from zo_adamm import ZO_AdaMM\n",
    "from zo_sgd import ZO_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ff9cde5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Example of a scale to get the parameters of networks between 1000 and 500000\n",
    "# start = 1./9\n",
    "# scale = torch.linspace(start,30,50)\n",
    "    \n",
    "class ModularModel(nn.Module):\n",
    "    def __init__(self,scale):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 3, 3)\n",
    "        self.fc1 = nn.Linear(3 * 5 * 5, max(10,int(scale*120)))\n",
    "        self.fc2 = nn.Linear(max(10,int(scale*120)), max(10,int(sqrt(scale*120))))\n",
    "        self.fc3 = nn.Linear(max(10,int(sqrt(scale*120))), 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "893e615e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN WITH  \"WORKING\" CLOSURE ON EFFICIENTNET\n",
    "\n",
    "def train(model, optimizer, criterion, nb_epochs, train_loader, test_loader):\n",
    "    # Heavily inspired from PyTorch tutorial\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    #global running_loss\n",
    "    running_loss = 0\n",
    "    lr_init = optimizer_pt.param_groups[0]['lr']\n",
    "    mu_init = optimizer_pt.param_groups[0]['mu']\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        ## change the learning rate alon iteration\n",
    "        #optimizer_pt.param_groups[0]['lr'] = max(lr_init/(np.sqrt(10)**(e/4.)),1e-5)\n",
    "        #optimizer_pt.param_groups[0]['mu'] = max(mu_init/(np.sqrt(10)**(e/4.)),1e-5)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            input, labels = data\n",
    "\n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "            ## initial evaluation\n",
    "            outputs = model(input)\n",
    "            loss_init = criterion(outputs, labels)\n",
    "\n",
    "            running_loss = running_loss + loss_init.item()\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "\n",
    "            def closure(size_params, mu):\n",
    "                grad_est = []\n",
    "\n",
    "\n",
    "\n",
    "                ## Generate a random direction uniformly on the unit ball or with a gaussian distribution\n",
    "                #u = torch.normal(mean = torch.zeros(size_params),std = 100)\n",
    "                u = 2*(torch.rand(size_params)-0.5) # need small modif in order to be on the unit sphere\n",
    "                u.div_(torch.norm(u,\"fro\"))\n",
    "\n",
    "                ## save the state of the model\n",
    "                model_init = dict(model.state_dict())\n",
    "                model_init_parameters = model.parameters()\n",
    "                grad_norm = 0\n",
    "\n",
    "                ## we add to the inital parameters a random perturbation times \\mu\n",
    "                start_ind = 0\n",
    "                for param_tensor in model.parameters():\n",
    "                    end_ind = start_ind + param_tensor.view(-1).size()[0]\n",
    "                    param_tensor.add_(u[start_ind:end_ind].view(param_tensor.size()).float(), alpha = mu)\n",
    "                    start_ind = end_ind\n",
    "\n",
    "                ## evaluation of the model and the with a random perturbation of the parameters\n",
    "                output2 = model(input)\n",
    "                loss_random = criterion(output2,labels)\n",
    "\n",
    "                ## compute the \"gradient norm\"\n",
    "\n",
    "                ## when u is uniform random variable\n",
    "                grad_norm = size_params*(loss_random-loss_init)/mu\n",
    "                ## when u is Gaussian random variable\n",
    "                #grad_norm += (loss_random-loss_init)/mu\n",
    "                #print(grad_norm,(loss_random-loss_init))\n",
    "\n",
    "                start_ind = 0\n",
    "                for param_tensor in model_init_parameters:\n",
    "                    end_ind = start_ind + param_tensor.view(-1).size()[0]\n",
    "                    grad_est.append((grad_norm/batch_size)*u[start_ind:end_ind].view(param_tensor.size()))\n",
    "                    start_ind = end_ind\n",
    "\n",
    "                ## reload initial state of the parameters\n",
    "                model.load_state_dict(model_init) # try to subtract the random vector to get back initial params\n",
    "\n",
    "                return grad_est\n",
    "\n",
    "            optimizer.step(closure)\n",
    "\n",
    "            if i % 20 == 19:\n",
    "                train_losses.append(running_loss / (100*20))#(batch_size*200))\n",
    "                print(f'epoch : {e + 1}/{nb_epochs} | train loss : {train_losses[-1]:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct_preds = 0\n",
    "            total_preds = 0\n",
    "\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                predictions = torch.argmax(outputs, 1)\n",
    "                total_preds += labels.size(0)\n",
    "                correct_preds += (predictions == labels).sum().item()\n",
    "\n",
    "            test_accuracies.append(correct_preds / total_preds)\n",
    "            print(f'epoch : {e + 1}/{nb_epochs} | test accuracies : {correct_preds / total_preds}')\n",
    "\n",
    "    return train_losses, test_accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a132a429",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ModularModel(nn.Module):\n",
    "    def __init__(self,scale):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 3, 3)\n",
    "        self.fc1 = nn.Linear(3 * 5 * 5, max(10,int(scale*120)))\n",
    "        self.fc2 = nn.Linear(max(10,int(scale*120)), 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e3432cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ModularModel(nn.Module):\n",
    "    def __init__(self,scale):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 3, 3)\n",
    "        self.fc1 = nn.Linear(3 * 5 * 5, max(10,int(scale*120)))\n",
    "        self.fc2 = nn.Linear(max(10,int(scale*120)), max(10,int(sqrt(scale*120))))\n",
    "        self.fc3 = nn.Linear(max(10,int(sqrt(scale*120))), 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c89638fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/40 | train loss : 0.0232\n",
      "epoch : 1/40 | train loss : 0.0231\n",
      "epoch : 1/40 | train loss : 0.0231\n",
      "epoch : 1/40 | train loss : 0.0230\n",
      "epoch : 1/40 | train loss : 0.0229\n",
      "epoch : 1/40 | train loss : 0.0228\n",
      "epoch : 1/40 | train loss : 0.0228\n",
      "epoch : 1/40 | train loss : 0.0228\n",
      "epoch : 1/40 | train loss : 0.0227\n",
      "epoch : 1/40 | train loss : 0.0227\n",
      "epoch : 1/40 | train loss : 0.0226\n",
      "epoch : 1/40 | train loss : 0.0225\n",
      "epoch : 1/40 | train loss : 0.0224\n",
      "epoch : 1/40 | train loss : 0.0224\n",
      "epoch : 1/40 | train loss : 0.0222\n",
      "epoch : 1/40 | train loss : 0.0221\n",
      "epoch : 1/40 | train loss : 0.0219\n",
      "epoch : 1/40 | train loss : 0.0217\n",
      "epoch : 1/40 | train loss : 0.0215\n",
      "epoch : 1/40 | train loss : 0.0213\n",
      "epoch : 1/40 | train loss : 0.0212\n",
      "epoch : 1/40 | train loss : 0.0211\n",
      "epoch : 1/40 | train loss : 0.0209\n",
      "epoch : 1/40 | train loss : 0.0205\n",
      "epoch : 1/40 | train loss : 0.0202\n",
      "epoch : 1/40 | train loss : 0.0201\n",
      "epoch : 1/40 | train loss : 0.0199\n",
      "epoch : 1/40 | train loss : 0.0196\n",
      "epoch : 1/40 | train loss : 0.0193\n",
      "epoch : 1/40 | train loss : 0.0189\n",
      "epoch : 1/40 | test accuracies : 0.36195\n",
      "epoch : 2/40 | train loss : 0.0189\n",
      "epoch : 2/40 | train loss : 0.0183\n",
      "epoch : 2/40 | train loss : 0.0182\n",
      "epoch : 2/40 | train loss : 0.0181\n",
      "epoch : 2/40 | train loss : 0.0175\n",
      "epoch : 2/40 | train loss : 0.0175\n",
      "epoch : 2/40 | train loss : 0.0176\n",
      "epoch : 2/40 | train loss : 0.0171\n",
      "epoch : 2/40 | train loss : 0.0171\n",
      "epoch : 2/40 | train loss : 0.0164\n",
      "epoch : 2/40 | train loss : 0.0165\n",
      "epoch : 2/40 | train loss : 0.0161\n",
      "epoch : 2/40 | train loss : 0.0159\n",
      "epoch : 2/40 | train loss : 0.0156\n",
      "epoch : 2/40 | train loss : 0.0155\n",
      "epoch : 2/40 | train loss : 0.0158\n",
      "epoch : 2/40 | train loss : 0.0153\n",
      "epoch : 2/40 | train loss : 0.0151\n",
      "epoch : 2/40 | train loss : 0.0148\n",
      "epoch : 2/40 | train loss : 0.0146\n",
      "epoch : 2/40 | train loss : 0.0147\n",
      "epoch : 2/40 | train loss : 0.0148\n",
      "epoch : 2/40 | train loss : 0.0143\n",
      "epoch : 2/40 | train loss : 0.0140\n",
      "epoch : 2/40 | train loss : 0.0139\n",
      "epoch : 2/40 | train loss : 0.0138\n",
      "epoch : 2/40 | train loss : 0.0137\n",
      "epoch : 2/40 | train loss : 0.0133\n",
      "epoch : 2/40 | train loss : 0.0133\n",
      "epoch : 2/40 | train loss : 0.0129\n",
      "epoch : 2/40 | test accuracies : 0.58285\n",
      "epoch : 3/40 | train loss : 0.0136\n",
      "epoch : 3/40 | train loss : 0.0130\n",
      "epoch : 3/40 | train loss : 0.0132\n",
      "epoch : 3/40 | train loss : 0.0134\n",
      "epoch : 3/40 | train loss : 0.0130\n",
      "epoch : 3/40 | train loss : 0.0131\n",
      "epoch : 3/40 | train loss : 0.0130\n",
      "epoch : 3/40 | train loss : 0.0129\n",
      "epoch : 3/40 | train loss : 0.0131\n",
      "epoch : 3/40 | train loss : 0.0121\n",
      "epoch : 3/40 | train loss : 0.0124\n",
      "epoch : 3/40 | train loss : 0.0124\n",
      "epoch : 3/40 | train loss : 0.0122\n",
      "epoch : 3/40 | train loss : 0.0120\n",
      "epoch : 3/40 | train loss : 0.0120\n",
      "epoch : 3/40 | train loss : 0.0127\n",
      "epoch : 3/40 | train loss : 0.0122\n",
      "epoch : 3/40 | train loss : 0.0120\n",
      "epoch : 3/40 | train loss : 0.0117\n",
      "epoch : 3/40 | train loss : 0.0116\n",
      "epoch : 3/40 | train loss : 0.0117\n",
      "epoch : 3/40 | train loss : 0.0120\n",
      "epoch : 3/40 | train loss : 0.0117\n",
      "epoch : 3/40 | train loss : 0.0113\n",
      "epoch : 3/40 | train loss : 0.0113\n",
      "epoch : 3/40 | train loss : 0.0113\n",
      "epoch : 3/40 | train loss : 0.0115\n",
      "epoch : 3/40 | train loss : 0.0110\n",
      "epoch : 3/40 | train loss : 0.0110\n",
      "epoch : 3/40 | train loss : 0.0103\n",
      "epoch : 3/40 | test accuracies : 0.6565166666666666\n",
      "epoch : 4/40 | train loss : 0.0111\n",
      "epoch : 4/40 | train loss : 0.0109\n",
      "epoch : 4/40 | train loss : 0.0110\n",
      "epoch : 4/40 | train loss : 0.0111\n",
      "epoch : 4/40 | train loss : 0.0110\n",
      "epoch : 4/40 | train loss : 0.0111\n",
      "epoch : 4/40 | train loss : 0.0111\n",
      "epoch : 4/40 | train loss : 0.0115\n",
      "epoch : 4/40 | train loss : 0.0116\n",
      "epoch : 4/40 | train loss : 0.0104\n",
      "epoch : 4/40 | train loss : 0.0109\n",
      "epoch : 4/40 | train loss : 0.0107\n",
      "epoch : 4/40 | train loss : 0.0106\n",
      "epoch : 4/40 | train loss : 0.0107\n",
      "epoch : 4/40 | train loss : 0.0103\n",
      "epoch : 4/40 | train loss : 0.0111\n",
      "epoch : 4/40 | train loss : 0.0104\n",
      "epoch : 4/40 | train loss : 0.0103\n",
      "epoch : 4/40 | train loss : 0.0099\n",
      "epoch : 4/40 | train loss : 0.0098\n",
      "epoch : 4/40 | train loss : 0.0101\n",
      "epoch : 4/40 | train loss : 0.0102\n",
      "epoch : 4/40 | train loss : 0.0101\n",
      "epoch : 4/40 | train loss : 0.0097\n",
      "epoch : 4/40 | train loss : 0.0096\n",
      "epoch : 4/40 | train loss : 0.0099\n",
      "epoch : 4/40 | train loss : 0.0100\n",
      "epoch : 4/40 | train loss : 0.0095\n",
      "epoch : 4/40 | train loss : 0.0094\n",
      "epoch : 4/40 | train loss : 0.0085\n",
      "epoch : 4/40 | test accuracies : 0.6839\n",
      "epoch : 5/40 | train loss : 0.0097\n",
      "epoch : 5/40 | train loss : 0.0093\n",
      "epoch : 5/40 | train loss : 0.0095\n",
      "epoch : 5/40 | train loss : 0.0096\n",
      "epoch : 5/40 | train loss : 0.0095\n",
      "epoch : 5/40 | train loss : 0.0094\n",
      "epoch : 5/40 | train loss : 0.0096\n",
      "epoch : 5/40 | train loss : 0.0103\n",
      "epoch : 5/40 | train loss : 0.0103\n",
      "epoch : 5/40 | train loss : 0.0088\n",
      "epoch : 5/40 | train loss : 0.0095\n",
      "epoch : 5/40 | train loss : 0.0097\n",
      "epoch : 5/40 | train loss : 0.0096\n",
      "epoch : 5/40 | train loss : 0.0097\n",
      "epoch : 5/40 | train loss : 0.0094\n",
      "epoch : 5/40 | train loss : 0.0102\n",
      "epoch : 5/40 | train loss : 0.0099\n",
      "epoch : 5/40 | train loss : 0.0095\n",
      "epoch : 5/40 | train loss : 0.0093\n",
      "epoch : 5/40 | train loss : 0.0092\n",
      "epoch : 5/40 | train loss : 0.0093\n",
      "epoch : 5/40 | train loss : 0.0094\n",
      "epoch : 5/40 | train loss : 0.0096\n",
      "epoch : 5/40 | train loss : 0.0092\n",
      "epoch : 5/40 | train loss : 0.0093\n",
      "epoch : 5/40 | train loss : 0.0093\n",
      "epoch : 5/40 | train loss : 0.0094\n",
      "epoch : 5/40 | train loss : 0.0091\n",
      "epoch : 5/40 | train loss : 0.0092\n",
      "epoch : 5/40 | train loss : 0.0079\n",
      "epoch : 5/40 | test accuracies : 0.6974666666666667\n",
      "epoch : 6/40 | train loss : 0.0092\n",
      "epoch : 6/40 | train loss : 0.0087\n",
      "epoch : 6/40 | train loss : 0.0086\n",
      "epoch : 6/40 | train loss : 0.0089\n",
      "epoch : 6/40 | train loss : 0.0090\n",
      "epoch : 6/40 | train loss : 0.0089\n",
      "epoch : 6/40 | train loss : 0.0093\n",
      "epoch : 6/40 | train loss : 0.0098\n",
      "epoch : 6/40 | train loss : 0.0097\n",
      "epoch : 6/40 | train loss : 0.0083\n",
      "epoch : 6/40 | train loss : 0.0089\n",
      "epoch : 6/40 | train loss : 0.0092\n",
      "epoch : 6/40 | train loss : 0.0090\n",
      "epoch : 6/40 | train loss : 0.0092\n",
      "epoch : 6/40 | train loss : 0.0087\n",
      "epoch : 6/40 | train loss : 0.0093\n",
      "epoch : 6/40 | train loss : 0.0091\n",
      "epoch : 6/40 | train loss : 0.0088\n",
      "epoch : 6/40 | train loss : 0.0088\n",
      "epoch : 6/40 | train loss : 0.0087\n",
      "epoch : 6/40 | train loss : 0.0089\n",
      "epoch : 6/40 | train loss : 0.0090\n",
      "epoch : 6/40 | train loss : 0.0091\n",
      "epoch : 6/40 | train loss : 0.0088\n",
      "epoch : 6/40 | train loss : 0.0090\n",
      "epoch : 6/40 | train loss : 0.0087\n",
      "epoch : 6/40 | train loss : 0.0089\n",
      "epoch : 6/40 | train loss : 0.0085\n",
      "epoch : 6/40 | train loss : 0.0083\n",
      "epoch : 6/40 | train loss : 0.0073\n",
      "epoch : 6/40 | test accuracies : 0.7202666666666667\n",
      "epoch : 7/40 | train loss : 0.0086\n",
      "epoch : 7/40 | train loss : 0.0082\n",
      "epoch : 7/40 | train loss : 0.0085\n",
      "epoch : 7/40 | train loss : 0.0085\n",
      "epoch : 7/40 | train loss : 0.0086\n",
      "epoch : 7/40 | train loss : 0.0085\n",
      "epoch : 7/40 | train loss : 0.0088\n",
      "epoch : 7/40 | train loss : 0.0092\n",
      "epoch : 7/40 | train loss : 0.0092\n",
      "epoch : 7/40 | train loss : 0.0080\n",
      "epoch : 7/40 | train loss : 0.0086\n",
      "epoch : 7/40 | train loss : 0.0086\n",
      "epoch : 7/40 | train loss : 0.0084\n",
      "epoch : 7/40 | train loss : 0.0086\n",
      "epoch : 7/40 | train loss : 0.0084\n",
      "epoch : 7/40 | train loss : 0.0091\n",
      "epoch : 7/40 | train loss : 0.0087\n",
      "epoch : 7/40 | train loss : 0.0084\n",
      "epoch : 7/40 | train loss : 0.0084\n",
      "epoch : 7/40 | train loss : 0.0083\n",
      "epoch : 7/40 | train loss : 0.0085\n",
      "epoch : 7/40 | train loss : 0.0085\n",
      "epoch : 7/40 | train loss : 0.0086\n",
      "epoch : 7/40 | train loss : 0.0084\n",
      "epoch : 7/40 | train loss : 0.0084\n",
      "epoch : 7/40 | train loss : 0.0081\n",
      "epoch : 7/40 | train loss : 0.0083\n",
      "epoch : 7/40 | train loss : 0.0080\n",
      "epoch : 7/40 | train loss : 0.0075\n",
      "epoch : 7/40 | train loss : 0.0067\n",
      "epoch : 7/40 | test accuracies : 0.7409333333333333\n",
      "epoch : 8/40 | train loss : 0.0079\n",
      "epoch : 8/40 | train loss : 0.0075\n",
      "epoch : 8/40 | train loss : 0.0075\n",
      "epoch : 8/40 | train loss : 0.0078\n",
      "epoch : 8/40 | train loss : 0.0079\n",
      "epoch : 8/40 | train loss : 0.0079\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-7f866c88a406>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0moptimizer_pt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mZO_AdaMM\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-03\u001B[0m\u001B[0;34m,\u001B[0m  \u001B[0mbetas\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.9\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.999\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmu\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;36m1e-03\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0mtrain_losses_pt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_acc_pt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer_pt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnb_epochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-11-ddbc457b1d6e>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, optimizer, criterion, nb_epochs, train_loader, test_loader)\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0;31m#optimizer_pt.param_groups[0]['lr'] = max(lr_init/(np.sqrt(10)**(e/4.)),1e-5)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m         \u001B[0;31m#optimizer_pt.param_groups[0]['mu'] = max(mu_init/(np.sqrt(10)**(e/4.)),1e-5)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m             \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    519\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    520\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 521\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    522\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    523\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    559\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    560\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 561\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    562\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    563\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 49\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     50\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 49\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     50\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 134\u001B[0;31m             \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    135\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_transform\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransforms\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m             \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, pic)\u001B[0m\n\u001B[1;32m     96\u001B[0m             \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mConverted\u001B[0m \u001B[0mimage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     97\u001B[0m         \"\"\"\n\u001B[0;32m---> 98\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001B[0m in \u001B[0;36mto_tensor\u001B[0;34m(pic)\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0mmode_to_nptype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m'I'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint32\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'I;16'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint16\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'F'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m     img = torch.from_numpy(\n\u001B[0;32m--> 141\u001B[0;31m         \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode_to_nptype\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muint8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m     )\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/PIL/Image.py\u001B[0m in \u001B[0;36m__array__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m    696\u001B[0m             \u001B[0mnew\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"data\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtobytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"raw\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"L\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    697\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 698\u001B[0;31m             \u001B[0mnew\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"data\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtobytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    699\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    700\u001B[0m         \u001B[0;32mclass\u001B[0m \u001B[0mArrayData\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = ModularModel(3)\n",
    "#model = torchvision.models.efficientnet_b0()\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=False)\n",
    "nb_epochs = 40\n",
    "\n",
    "with torch.no_grad():\n",
    "    optimizer_pt = ZO_AdaMM(model.parameters(), lr=1e-03,  betas=(0.9, 0.999),mu =1e-03, eps=1e-10)\n",
    "    \n",
    "    train_losses_pt, test_acc_pt = train(model, optimizer_pt, criterion, nb_epochs, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfa15e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses_pt, label='Ours AdaMM')\n",
    "plt.title('Train losses (avg over 2000 batches)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ebc5c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(test_acc_pt, label='Ours AdaMM')\n",
    "plt.title('Test accuracies')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932fc0bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, nb_epochs, train_loader, test_loader):\n",
    "    # Heavily inspired from PyTorch tutorial\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            #print(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            #print(loss)\n",
    "            #print('-----------------')\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 2000 == 1999:\n",
    "                train_losses.append(running_loss / 1000)\n",
    "                print(f'epoch : {e + 1}/{nb_epochs} | train loss : {train_losses[-1]:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct_preds = 0\n",
    "            total_preds = 0\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                predictions = torch.argmax(outputs, 1)\n",
    "                total_preds += labels.size(0)\n",
    "                correct_preds += (predictions == labels).sum().item()\n",
    "\n",
    "            test_accuracies.append(correct_preds / total_preds)\n",
    "\n",
    "    return train_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c80e0b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_model = SmallModel()\n",
    "\n",
    "optimizer_pt = optim.Adam(small_model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8, amsgrad=True)\n",
    "\n",
    "train_losses_pt, test_acc_pt = train(small_model, optimizer_pt, criterion, nb_epochs, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a205f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_acc_pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}