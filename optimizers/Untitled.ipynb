{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d999ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e819a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure(size_params,mu):\n",
    "    grad_est = []\n",
    "    \n",
    "    u = torch.normal(mean = torch.zeros(size_params),std = 1)\n",
    "    u = torch.div(u,torch.norm(u,\"fro\"))\n",
    "    \n",
    "    # initial evaluation\n",
    "    output = model(input)\n",
    "    loss_init = criterion.forward(output)\n",
    "    \n",
    "    # save the state of the model \n",
    "    model_init = dict(model.state_dict())\n",
    "    \n",
    "    start_ind = 0\n",
    "    for param_tensor in model.state_dict():\n",
    "        end_ind = start + model.state_dict()[param_tensor].view(-1).size()[0]\n",
    "        model.state_dict()[param_tensor].add_(u[start_ind:end_ind].view(model.state_dict()[param_tensor].size()), value = mu)\n",
    "    \n",
    "    # random evaluation\n",
    "    output2 = model(input)\n",
    "    loss_random = criterion.forward(output2)\n",
    "    \n",
    "    \n",
    "    # load initial state\n",
    "    model.load_state_dict(model_init)\n",
    "    \n",
    "    # compute the gradient\n",
    "    \n",
    "    grad_norm = size_params*(loss_random-loss_init)/mu\n",
    "    grad_est = []\n",
    "    \n",
    "    start_ind = 0\n",
    "    for param_tensor in model_init:\n",
    "        end_ind = start + model_init[param_tensor].view(-1).size()[0]\n",
    "        grad_est.apppend(grad_norm*u[start_ind:end_ind].view(model_init[param_tensor].size()))\n",
    "    \n",
    "    return grad_est\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42659f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZO_AdaMM(Optimizer):\n",
    "    \n",
    "    def __init__(self,params,lr = 1e-03,betas = (0.9,0.999), mu = 1e-05, eps = 1e-12):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: (} - should be >= 0.0\". format (lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError (\"Invalid beta parameter: (} - should be in [0.0, 1.0[\". format (betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0l\". format (betas [1]))\n",
    "        if not 0.0 <= mu < 1.0:\n",
    "            raise ValueError(\"Invalid mu parameter: {} - should be in [0.0, 1.0l\". format (mu))\n",
    "            \n",
    "        defaults = dict(lr=lr, betas=betas, mu=mu, eps = eps)\n",
    "        super(ZO_AdaMM,self).__init__(params,defaults)\n",
    "        \n",
    "    def step(self, closure):\n",
    "        \n",
    "         for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            max_exp_avg_sqs = []\n",
    "            state_steps = []\n",
    "            beta1, beta2 = group['betas']\n",
    "            \n",
    "            size_params = 0\n",
    "            \n",
    "            for p in group['params']:\n",
    "                size_params += p.view(-1).size()[0]\n",
    "            \n",
    "            # closure return the approximation for the gradient, we have to add some \"option\" to this function \n",
    "            grad_est = closure(size_params,group[\"mu\"])\n",
    "            \n",
    "            i = 0\n",
    "            for p in group['params']:    \n",
    "                #grads.append(grad_est[i])\n",
    "                state = self.state[p]\n",
    "                # Lazy state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    #if group['amsgrad']:\n",
    "                    # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                    state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                exp_avgs.append(state['exp_avg'])\n",
    "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "\n",
    "                #if group['amsgrad']:\n",
    "                max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
    "\n",
    "                # update the steps for each param group update\n",
    "                state['step'] += 1\n",
    "                # record the step after step update\n",
    "                state_steps.append(state['step'])\n",
    "                    \n",
    "                \n",
    "                beta1, beta2 = group['betas']\n",
    "                state['exp_avg'].mul_(beta1).add_(grad_est[i],alpha = (1.0 - beta1))\n",
    "                state['exp_avg_sq'].mul_(beta2).addcmul_(grad_est[i], grad_est[i],value = (1.0 - beta2))\n",
    "                state['max_exp_avg_sq'] = torch.max(state['max_exp_avg_sq'],state['exp_avg_sq'])# vÃ©rifier max ou maximum\n",
    "                \n",
    "                p.data.addcdiv_(state['exp_avg'], state['exp_avg_sq'].sqrt().add_(group['eps']),value = (-group['lr']))\n",
    "                i +=1\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db7933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901ed4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,12)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93585af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 9, 3)\n",
    "        #self.fc1 = nn.Linear(9 * 5 * 5, 15)\n",
    "        #self.fc2 = nn.Linear(15, 13)\n",
    "        self.fc3 = nn.Linear(9 * 5 * 5, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5acd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "mnist_dataset_train = torchvision.datasets.MNIST('data/mnist/', download=True, train=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dataset_train, batch_size=1)\n",
    "\n",
    "mnist_dataset_test = torchvision.datasets.MNIST('data/mnist/', download=True, train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_dataset_train, batch_size=1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "nb_epochs = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "44ffd7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, nb_epochs, train_loader, test_loader):\n",
    "    # Heavily inspired from PyTorch tutorial\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "                \n",
    "            def closure(size_params,mu):\n",
    "                # initial evaluation\n",
    "                output = outputs\n",
    "                loss_init = loss \n",
    "                # save the state of the model \n",
    "                model_init = dict(model.state_dict())\n",
    "                #u = torch.normal(mean = torch.zeros(size_params),std = 1)\n",
    "                u = 2*(torch.rand(size_params)-0.5)\n",
    "                u.div_(torch.norm(u,\"fro\"))\n",
    "\n",
    "                start_ind = 0\n",
    "                for param_tensor in model.state_dict():\n",
    "                    end_ind = start_ind + model.state_dict()[param_tensor].view(-1).size()[0]\n",
    "                    model.state_dict()[param_tensor].add_(u[start_ind:end_ind].view(model.state_dict()[param_tensor].size()), alpha = mu)\n",
    "                    start_ind = end_ind\n",
    "                print(type(model.state_dict()))\n",
    "                # random evaluation\n",
    "                output2 = model(inputs)\n",
    "                loss_random = criterion(output2,labels)\n",
    "\n",
    "\n",
    "                # load initial state\n",
    "                model.load_state_dict(model_init)\n",
    "                # compute the gradient\n",
    "                \n",
    "                # when u is uniform random variable\n",
    "                grad_norm = size_params*(loss_random-loss_init)/mu\n",
    "                # when u is Gaussian random variable\n",
    "                #grad_norm = (loss_random-loss_init)/mu\n",
    "                grad_est = []\n",
    "\n",
    "                start_ind = 0\n",
    "                for param_tensor in model_init:\n",
    "                    end_ind = start_ind + model_init[param_tensor].view(-1).size()[0]\n",
    "                    grad_est.append(grad_norm*u[start_ind:end_ind].view(model_init[param_tensor].size()))\n",
    "                    start_ind = end_ind\n",
    "                return grad_est\n",
    "     \n",
    "            \n",
    "            optimizer.step(closure)\n",
    "\n",
    "            if i % 2000 == 1999:\n",
    "                train_losses.append(running_loss / 2000)\n",
    "                print(f'epoch : {e + 1}/{nb_epochs} | train loss : {train_losses[-1]:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct_preds = 0\n",
    "            total_preds = 0\n",
    "\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                predictions = torch.argmax(outputs, 1)\n",
    "                total_preds += labels.size(0)\n",
    "                correct_preds += (predictions == labels).sum().item()\n",
    "\n",
    "            test_accuracies.append(correct_preds / total_preds)\n",
    "\n",
    "\n",
    "    return train_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa7687da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 5, 5])\n",
      "torch.Size([3])\n",
      "torch.Size([9, 3, 3, 3])\n",
      "torch.Size([9])\n",
      "torch.Size([10, 225])\n",
      "torch.Size([10])\n",
      "torch.Size([3, 1, 5, 5])\n",
      "torch.Size([3])\n",
      "torch.Size([9, 3, 3, 3])\n",
      "torch.Size([9])\n",
      "torch.Size([10, 225])\n",
      "torch.Size([10])\n",
      "torch.Size([3, 1, 5, 5])\n",
      "torch.Size([3])\n",
      "torch.Size([9, 3, 3, 3])\n",
      "torch.Size([9])\n",
      "torch.Size([10, 225])\n",
      "torch.Size([10])\n",
      "torch.Size([3, 1, 5, 5])\n",
      "torch.Size([3])\n",
      "torch.Size([9, 3, 3, 3])\n",
      "torch.Size([9])\n",
      "torch.Size([10, 225])\n",
      "torch.Size([10])\n",
      "torch.Size([3, 1, 5, 5])\n",
      "torch.Size([3])\n",
      "torch.Size([9, 3, 3, 3])\n",
      "torch.Size([9])\n",
      "torch.Size([10, 225])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for param_tensor in model.state_dict().values(): \n",
    "        print(param_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "893e615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'collections.OrderedDict'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_92947/4137395464.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moptimizer_pt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZO_AdaMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1e-03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_losses_pt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc_pt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_pt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_92947/2272049570.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, nb_epochs, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m#optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_92947/500939728.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1174\u001b[0m             \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_modules'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SmallModel()\n",
    "with torch.no_grad():\n",
    "    optimizer_pt = ZO_AdaMM(model.parameters(), lr=1e-3, betas=(0.3, 0.9),mu =1e-03, eps=1e-10)\n",
    "\n",
    "    train_losses_pt, test_acc_pt = train(model, optimizer_pt, criterion, nb_epochs, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "932fc0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, nb_epochs, train_loader, test_loader):\n",
    "    # Heavily inspired from PyTorch tutorial\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            #print(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            #print(loss)\n",
    "            #print('-----------------')\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 2000 == 1999:\n",
    "                train_losses.append(running_loss / 1000)\n",
    "                print(f'epoch : {e + 1}/{nb_epochs} | train loss : {train_losses[-1]:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct_preds = 0\n",
    "            total_preds = 0\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                predictions = torch.argmax(outputs, 1)\n",
    "                total_preds += labels.size(0)\n",
    "                correct_preds += (predictions == labels).sum().item()\n",
    "\n",
    "            test_accuracies.append(correct_preds / total_preds)\n",
    "\n",
    "    return train_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a2c80e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/4 | train loss : 1.4947\n",
      "epoch : 1/4 | train loss : 0.5242\n",
      "epoch : 1/4 | train loss : 0.4342\n",
      "epoch : 1/4 | train loss : 0.3548\n",
      "epoch : 1/4 | train loss : 0.3857\n",
      "epoch : 1/4 | train loss : 0.2623\n",
      "epoch : 1/4 | train loss : 0.3025\n",
      "epoch : 1/4 | train loss : 0.2995\n",
      "epoch : 1/4 | train loss : 0.2553\n",
      "epoch : 1/4 | train loss : 0.2043\n",
      "epoch : 1/4 | train loss : 0.2413\n",
      "epoch : 1/4 | train loss : 0.2041\n",
      "epoch : 1/4 | train loss : 0.1924\n",
      "epoch : 1/4 | train loss : 0.2067\n",
      "epoch : 1/4 | train loss : 0.1955\n",
      "epoch : 1/4 | train loss : 0.2156\n",
      "epoch : 1/4 | train loss : 0.2010\n",
      "epoch : 1/4 | train loss : 0.1986\n",
      "epoch : 1/4 | train loss : 0.1985\n",
      "epoch : 1/4 | train loss : 0.1803\n",
      "epoch : 1/4 | train loss : 0.2019\n",
      "epoch : 1/4 | train loss : 0.2035\n",
      "epoch : 1/4 | train loss : 0.1844\n",
      "epoch : 1/4 | train loss : 0.2311\n",
      "epoch : 1/4 | train loss : 0.1943\n",
      "epoch : 1/4 | train loss : 0.1611\n",
      "epoch : 1/4 | train loss : 0.2041\n",
      "epoch : 1/4 | train loss : 0.1393\n",
      "epoch : 1/4 | train loss : 0.1210\n",
      "epoch : 1/4 | train loss : 0.1178\n",
      "OrderedDict([('conv1.weight', tensor([[[[ 3.7517e-01,  2.9128e-01,  3.4534e-01, -3.2848e-02, -8.9228e-02],\n",
      "          [ 2.9452e-01,  2.1370e-01,  2.0059e-01, -9.8000e-02, -2.4706e-01],\n",
      "          [ 4.6744e-01,  3.4016e-01,  2.9211e-02, -5.2809e-01, -3.4434e-01],\n",
      "          [ 4.9071e-01,  3.2504e-01, -2.3455e-01, -5.1469e-01, -3.3390e-01],\n",
      "          [ 3.4325e-01,  3.0162e-01, -7.1674e-02, -2.7099e-01, -2.7417e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.1672e-01, -1.5166e-01,  1.6489e-01,  8.3163e-02, -1.8860e-01],\n",
      "          [-3.3268e-01, -1.9002e-02,  4.6302e-01,  4.7239e-01,  3.4000e-01],\n",
      "          [-9.8284e-02,  2.0527e-03,  2.9114e-01,  7.1394e-01,  3.5723e-01],\n",
      "          [ 4.6399e-02,  2.1948e-01,  5.6286e-01,  6.2013e-01,  1.8109e-01],\n",
      "          [ 1.0544e-01,  3.7013e-01,  2.8313e-01,  2.1934e-01, -6.4573e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4852e-01, -2.2971e-02,  6.2029e-03, -2.3773e-02,  2.2624e-01],\n",
      "          [ 2.2126e-01,  5.2742e-01,  5.6354e-01,  3.4026e-01,  3.9441e-01],\n",
      "          [-1.3399e-01,  4.0622e-01,  3.9905e-01,  3.7535e-01,  6.2029e-01],\n",
      "          [-4.0892e-01, -3.7237e-01, -1.9474e-01,  1.9375e-04,  3.7450e-01],\n",
      "          [-7.2513e-01, -7.5011e-01, -7.3028e-01, -5.8545e-01, -2.1835e-01]]]])), ('conv1.bias', tensor([-0.0154, -0.0204,  0.0334])), ('conv2.weight', tensor([[[[-0.0380,  0.0284, -0.2830],\n",
      "          [-0.1548,  0.0281,  0.0604],\n",
      "          [-0.2166, -0.3955,  0.0190]],\n",
      "\n",
      "         [[ 0.2375, -0.3084, -0.4542],\n",
      "          [ 0.0584,  0.1444, -0.0711],\n",
      "          [ 0.2923,  0.0272,  0.1939]],\n",
      "\n",
      "         [[-0.1066, -0.3455, -0.0866],\n",
      "          [ 0.1454,  0.1459, -0.2318],\n",
      "          [ 0.1788,  0.3442,  0.0887]]],\n",
      "\n",
      "\n",
      "        [[[-0.7733, -0.4684, -0.3573],\n",
      "          [-0.1049, -0.4350, -0.0368],\n",
      "          [ 0.3158,  0.1328,  0.1020]],\n",
      "\n",
      "         [[-0.2072, -0.2327, -0.0922],\n",
      "          [ 0.2589, -0.0681,  0.1245],\n",
      "          [ 0.0110, -0.0892,  0.1083]],\n",
      "\n",
      "         [[-0.6854, -0.7672, -0.8589],\n",
      "          [ 0.0917,  0.0408,  0.2905],\n",
      "          [ 0.4380,  0.1534,  0.2121]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1071, -0.1033, -0.3515],\n",
      "          [ 0.0686, -0.5809,  0.0984],\n",
      "          [ 0.1992, -0.1837,  0.7051]],\n",
      "\n",
      "         [[-0.5262, -0.0405,  0.4253],\n",
      "          [-0.3334,  0.2817,  0.1530],\n",
      "          [-0.2705,  0.1538,  0.2455]],\n",
      "\n",
      "         [[-0.4048, -0.4128, -0.2962],\n",
      "          [-0.4735, -0.0312, -0.4705],\n",
      "          [-0.3072, -0.0576, -0.6814]]],\n",
      "\n",
      "\n",
      "        [[[-0.0991,  0.1095, -0.0816],\n",
      "          [ 0.0972, -0.0757,  0.2068],\n",
      "          [ 0.0249,  0.0446, -0.1551]],\n",
      "\n",
      "         [[ 0.3034,  0.1548,  0.4212],\n",
      "          [-0.0776,  0.1207, -0.2114],\n",
      "          [-0.3134, -0.2591, -0.6462]],\n",
      "\n",
      "         [[-0.0733, -0.2423,  0.0647],\n",
      "          [ 0.3708,  0.3653,  0.1517],\n",
      "          [ 0.1404, -0.1491, -0.0249]]],\n",
      "\n",
      "\n",
      "        [[[-0.1103,  0.2414,  0.2702],\n",
      "          [ 0.2495,  0.3165,  0.0867],\n",
      "          [ 0.2276,  0.0214, -0.1080]],\n",
      "\n",
      "         [[ 0.2480,  0.0783,  0.0457],\n",
      "          [ 0.1372, -0.2245, -0.2268],\n",
      "          [ 0.1405, -0.0424, -0.1200]],\n",
      "\n",
      "         [[-0.1656, -0.2646, -0.1864],\n",
      "          [-0.4774, -0.3627, -0.0353],\n",
      "          [-0.5014, -0.4149, -0.0105]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6044,  0.2667, -0.3353],\n",
      "          [ 0.2920,  0.1867, -0.0718],\n",
      "          [-0.0027, -0.2851,  0.2426]],\n",
      "\n",
      "         [[ 0.1751, -0.6334,  0.0916],\n",
      "          [-0.4349, -0.0993,  0.2261],\n",
      "          [-0.3451,  0.0020,  0.1537]],\n",
      "\n",
      "         [[-0.0725, -0.5132, -0.0306],\n",
      "          [-0.2787,  0.0748, -0.0723],\n",
      "          [-0.5394, -0.2062,  0.1001]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2479, -0.1259, -0.2893],\n",
      "          [ 0.3720, -0.0704, -0.4697],\n",
      "          [ 0.3178,  0.1730, -0.2225]],\n",
      "\n",
      "         [[-0.3188, -0.3831,  0.2220],\n",
      "          [-0.1335, -0.4632,  0.3182],\n",
      "          [ 0.1286, -0.3865, -0.0778]],\n",
      "\n",
      "         [[-0.2318, -0.0294,  0.2493],\n",
      "          [-0.5719,  0.0362,  0.4403],\n",
      "          [-0.5241, -0.2732,  0.4078]]],\n",
      "\n",
      "\n",
      "        [[[-0.3586, -0.0107,  0.6169],\n",
      "          [-0.4254, -0.7164,  0.3265],\n",
      "          [-0.8312, -0.2560,  0.1768]],\n",
      "\n",
      "         [[ 0.2999,  0.0709, -0.1824],\n",
      "          [-0.1114,  0.2797, -0.2315],\n",
      "          [-0.1838,  0.2643,  0.0359]],\n",
      "\n",
      "         [[ 0.2539, -0.0653, -0.8996],\n",
      "          [ 0.3221, -0.0389, -0.3180],\n",
      "          [-0.0971, -0.0563,  0.2213]]],\n",
      "\n",
      "\n",
      "        [[[-0.0699,  0.0603, -0.1846],\n",
      "          [-0.4219, -0.1521, -0.2762],\n",
      "          [-0.4993, -0.3952, -0.2839]],\n",
      "\n",
      "         [[ 0.1273, -0.1346, -0.0711],\n",
      "          [-0.1095, -0.1160, -0.1207],\n",
      "          [-0.3109, -0.0941, -0.1680]],\n",
      "\n",
      "         [[ 0.2211,  0.1870, -0.2028],\n",
      "          [ 0.2516,  0.4419,  0.6385],\n",
      "          [ 0.0822,  0.0441,  0.4544]]]])), ('conv2.bias', tensor([ 0.0113,  0.0208, -0.1662, -0.0571, -0.2658, -0.1143,  0.0485, -0.4931,\n",
      "        -0.1036])), ('fc3.weight', tensor([[-0.0860,  0.0264,  0.1410,  ...,  0.0340, -0.0015, -0.1545],\n",
      "        [-0.1400, -0.1199, -0.0338,  ..., -0.0185,  0.0781,  0.1075],\n",
      "        [-0.1479, -0.0961,  0.0076,  ...,  0.1294,  0.1689,  0.2194],\n",
      "        ...,\n",
      "        [ 0.2437,  0.1071,  0.0836,  ..., -0.0512, -0.3108, -0.1257],\n",
      "        [ 0.0363, -0.0603, -0.1424,  ...,  0.0067,  0.0417, -0.0701],\n",
      "        [-0.0692, -0.0074,  0.1595,  ..., -0.0633, -0.0607, -0.0397]])), ('fc3.bias', tensor([-0.0413, -0.0456,  0.0964,  0.0589, -0.0481,  0.0103,  0.0039, -0.0126,\n",
      "         0.0464,  0.0357]))])\n",
      "epoch : 2/4 | train loss : 0.2053\n",
      "epoch : 2/4 | train loss : 0.1612\n",
      "epoch : 2/4 | train loss : 0.1436\n",
      "epoch : 2/4 | train loss : 0.1609\n",
      "epoch : 2/4 | train loss : 0.1728\n",
      "epoch : 2/4 | train loss : 0.1231\n",
      "epoch : 2/4 | train loss : 0.1437\n",
      "epoch : 2/4 | train loss : 0.1617\n",
      "epoch : 2/4 | train loss : 0.1403\n",
      "epoch : 2/4 | train loss : 0.1071\n",
      "epoch : 2/4 | train loss : 0.1451\n",
      "epoch : 2/4 | train loss : 0.1328\n",
      "epoch : 2/4 | train loss : 0.1372\n",
      "epoch : 2/4 | train loss : 0.1678\n",
      "epoch : 2/4 | train loss : 0.1073\n",
      "epoch : 2/4 | train loss : 0.1327\n",
      "epoch : 2/4 | train loss : 0.1352\n",
      "epoch : 2/4 | train loss : 0.1463\n",
      "epoch : 2/4 | train loss : 0.1461\n",
      "epoch : 2/4 | train loss : 0.1396\n",
      "epoch : 2/4 | train loss : 0.1430\n",
      "epoch : 2/4 | train loss : 0.1626\n",
      "epoch : 2/4 | train loss : 0.1325\n",
      "epoch : 2/4 | train loss : 0.1645\n",
      "epoch : 2/4 | train loss : 0.1470\n",
      "epoch : 2/4 | train loss : 0.1206\n",
      "epoch : 2/4 | train loss : 0.1408\n",
      "epoch : 2/4 | train loss : 0.1043\n",
      "epoch : 2/4 | train loss : 0.0911\n",
      "epoch : 2/4 | train loss : 0.0978\n",
      "OrderedDict([('conv1.weight', tensor([[[[ 0.3008,  0.2377,  0.3429,  0.0115, -0.0889],\n",
      "          [ 0.2906,  0.2198,  0.1998, -0.0732, -0.2845],\n",
      "          [ 0.5263,  0.4436,  0.0452, -0.5432, -0.2915],\n",
      "          [ 0.5080,  0.3739, -0.2379, -0.5298, -0.2413],\n",
      "          [ 0.3518,  0.3202, -0.0471, -0.2283, -0.2274]]],\n",
      "\n",
      "\n",
      "        [[[-0.2694, -0.2308,  0.1355,  0.0958, -0.3350],\n",
      "          [-0.5400, -0.0467,  0.4779,  0.5814,  0.3077],\n",
      "          [-0.3298, -0.1213,  0.2687,  0.8746,  0.4349],\n",
      "          [-0.1061,  0.1461,  0.5785,  0.6418,  0.1435],\n",
      "          [ 0.1113,  0.3873,  0.2416,  0.1267, -0.0946]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1843,  0.0397,  0.0215, -0.0461,  0.0853],\n",
      "          [ 0.2273,  0.5994,  0.6217,  0.3714,  0.3048],\n",
      "          [-0.1971,  0.3978,  0.4590,  0.4116,  0.6081],\n",
      "          [-0.5028, -0.4211, -0.1739,  0.0102,  0.4257],\n",
      "          [-0.7692, -0.7884, -0.7267, -0.6698, -0.2177]]]])), ('conv1.bias', tensor([-0.0204, -0.0632,  0.0513])), ('conv2.weight', tensor([[[[-0.0427,  0.0899, -0.3453],\n",
      "          [-0.1518, -0.0183,  0.0799],\n",
      "          [-0.1922, -0.4363,  0.0671]],\n",
      "\n",
      "         [[ 0.2327, -0.3046, -0.4641],\n",
      "          [ 0.0595,  0.1322, -0.0815],\n",
      "          [ 0.3266,  0.0607,  0.1305]],\n",
      "\n",
      "         [[-0.2188, -0.4727, -0.0441],\n",
      "          [ 0.2826,  0.1433, -0.2239],\n",
      "          [ 0.1634,  0.3625,  0.1035]]],\n",
      "\n",
      "\n",
      "        [[[-0.8761, -0.4987, -0.3965],\n",
      "          [-0.0321, -0.4579, -0.1194],\n",
      "          [ 0.3437,  0.0560,  0.0458]],\n",
      "\n",
      "         [[-0.2072, -0.2819, -0.1730],\n",
      "          [ 0.3360, -0.0118,  0.1197],\n",
      "          [ 0.0622, -0.0365,  0.1755]],\n",
      "\n",
      "         [[-0.6844, -0.8572, -0.9353],\n",
      "          [ 0.0733,  0.0372,  0.3013],\n",
      "          [ 0.4320,  0.1258,  0.1847]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0438, -0.1993, -0.4317],\n",
      "          [ 0.0029, -0.6336,  0.0495],\n",
      "          [ 0.1665, -0.2262,  0.6591]],\n",
      "\n",
      "         [[-0.6276, -0.0637,  0.4804],\n",
      "          [-0.4156,  0.3338,  0.2237],\n",
      "          [-0.3588,  0.1845,  0.2574]],\n",
      "\n",
      "         [[-0.4114, -0.5450, -0.4306],\n",
      "          [-0.4754, -0.0492, -0.5957],\n",
      "          [-0.3046, -0.0504, -0.8306]]],\n",
      "\n",
      "\n",
      "        [[[-0.0694,  0.0581, -0.0098],\n",
      "          [ 0.0759, -0.0304,  0.2405],\n",
      "          [-0.0173,  0.0493, -0.2356]],\n",
      "\n",
      "         [[ 0.2521,  0.1944,  0.4793],\n",
      "          [-0.1169,  0.1259, -0.1915],\n",
      "          [-0.3379, -0.3083, -0.6720]],\n",
      "\n",
      "         [[-0.0358, -0.2579, -0.0596],\n",
      "          [ 0.3996,  0.4603,  0.0972],\n",
      "          [ 0.1617, -0.1443, -0.0970]]],\n",
      "\n",
      "\n",
      "        [[[-0.0527,  0.3325,  0.2532],\n",
      "          [ 0.2571,  0.3028,  0.1005],\n",
      "          [ 0.1715, -0.1140, -0.1901]],\n",
      "\n",
      "         [[ 0.3422,  0.0708,  0.0574],\n",
      "          [ 0.2051, -0.2387, -0.2815],\n",
      "          [ 0.0829, -0.0876, -0.1803]],\n",
      "\n",
      "         [[-0.1779, -0.2141, -0.1682],\n",
      "          [-0.5203, -0.3616, -0.0744],\n",
      "          [-0.5302, -0.3867, -0.0055]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6407,  0.2972, -0.3621],\n",
      "          [ 0.2944,  0.1696, -0.0983],\n",
      "          [-0.1616, -0.4021,  0.1687]],\n",
      "\n",
      "         [[ 0.2578, -0.6425,  0.0259],\n",
      "          [-0.4712, -0.1082,  0.2684],\n",
      "          [-0.3751,  0.0260,  0.2151]],\n",
      "\n",
      "         [[-0.0610, -0.5572, -0.1975],\n",
      "          [-0.1398,  0.0572, -0.1457],\n",
      "          [-0.5315, -0.1912,  0.0838]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1759, -0.1332, -0.2795],\n",
      "          [ 0.3955, -0.0685, -0.4635],\n",
      "          [ 0.3971,  0.2061, -0.2367]],\n",
      "\n",
      "         [[-0.3725, -0.4749,  0.2590],\n",
      "          [-0.1924, -0.5878,  0.3586],\n",
      "          [ 0.1467, -0.5126, -0.1189]],\n",
      "\n",
      "         [[-0.1322, -0.0559,  0.1635],\n",
      "          [-0.6905, -0.0653,  0.4149],\n",
      "          [-0.5614, -0.3728,  0.5141]]],\n",
      "\n",
      "\n",
      "        [[[-0.3424, -0.0125,  0.7041],\n",
      "          [-0.5636, -0.8426,  0.3273],\n",
      "          [-0.9871, -0.3332,  0.1353]],\n",
      "\n",
      "         [[ 0.3407,  0.1035, -0.3262],\n",
      "          [-0.1331,  0.3251, -0.3525],\n",
      "          [-0.2867,  0.3094,  0.0406]],\n",
      "\n",
      "         [[ 0.2341, -0.1236, -0.9596],\n",
      "          [ 0.2728, -0.0400, -0.3680],\n",
      "          [-0.1317, -0.0374,  0.2906]]],\n",
      "\n",
      "\n",
      "        [[[-0.0783,  0.0495, -0.2164],\n",
      "          [-0.5611, -0.2131, -0.3952],\n",
      "          [-0.6202, -0.4146, -0.2694]],\n",
      "\n",
      "         [[ 0.1056, -0.1855, -0.1368],\n",
      "          [-0.0688, -0.1316, -0.1864],\n",
      "          [-0.2493, -0.0885, -0.2237]],\n",
      "\n",
      "         [[ 0.3070,  0.1383, -0.3002],\n",
      "          [ 0.3121,  0.4892,  0.7066],\n",
      "          [-0.0457, -0.0773,  0.4300]]]])), ('conv2.bias', tensor([-0.0166, -0.0416, -0.0656, -0.0223, -0.3013, -0.1685,  0.0451, -0.6636,\n",
      "        -0.0204])), ('fc3.weight', tensor([[-0.0743,  0.0395,  0.1561,  ...,  0.0765,  0.0304, -0.2146],\n",
      "        [-0.1151, -0.0918, -0.0564,  ..., -0.0856,  0.0917,  0.1626],\n",
      "        [-0.1961, -0.1022,  0.0584,  ...,  0.0970,  0.2264,  0.3415],\n",
      "        ...,\n",
      "        [ 0.3337,  0.1234,  0.0774,  ..., -0.1004, -0.2614, -0.0903],\n",
      "        [-0.0072, -0.0960, -0.1405,  ...,  0.0142,  0.0812, -0.1178],\n",
      "        [-0.0916, -0.0241,  0.1489,  ..., -0.0484, -0.1646, -0.0317]])), ('fc3.bias', tensor([-0.0319, -0.0679,  0.1334,  0.0685, -0.0873,  0.0124, -0.0090, -0.0279,\n",
      "         0.1089,  0.0043]))])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 3/4 | train loss : 0.1726\n",
      "epoch : 3/4 | train loss : 0.1304\n",
      "epoch : 3/4 | train loss : 0.1199\n",
      "epoch : 3/4 | train loss : 0.1297\n",
      "epoch : 3/4 | train loss : 0.1362\n",
      "epoch : 3/4 | train loss : 0.1054\n",
      "epoch : 3/4 | train loss : 0.1185\n",
      "epoch : 3/4 | train loss : 0.1301\n",
      "epoch : 3/4 | train loss : 0.1107\n",
      "epoch : 3/4 | train loss : 0.0904\n",
      "epoch : 3/4 | train loss : 0.1172\n",
      "epoch : 3/4 | train loss : 0.1214\n",
      "epoch : 3/4 | train loss : 0.1180\n",
      "epoch : 3/4 | train loss : 0.1560\n",
      "epoch : 3/4 | train loss : 0.0894\n",
      "epoch : 3/4 | train loss : 0.1153\n",
      "epoch : 3/4 | train loss : 0.1170\n",
      "epoch : 3/4 | train loss : 0.1265\n",
      "epoch : 3/4 | train loss : 0.1283\n",
      "epoch : 3/4 | train loss : 0.1150\n",
      "epoch : 3/4 | train loss : 0.1178\n",
      "epoch : 3/4 | train loss : 0.1413\n",
      "epoch : 3/4 | train loss : 0.1132\n",
      "epoch : 3/4 | train loss : 0.1359\n",
      "epoch : 3/4 | train loss : 0.1341\n",
      "epoch : 3/4 | train loss : 0.1023\n",
      "epoch : 3/4 | train loss : 0.1182\n",
      "epoch : 3/4 | train loss : 0.0929\n",
      "epoch : 3/4 | train loss : 0.0802\n",
      "epoch : 3/4 | train loss : 0.0885\n",
      "OrderedDict([('conv1.weight', tensor([[[[ 0.2441,  0.2168,  0.3444,  0.0350, -0.0802],\n",
      "          [ 0.2332,  0.2186,  0.1829, -0.0947, -0.3198],\n",
      "          [ 0.4942,  0.4905,  0.0297, -0.5803, -0.2557],\n",
      "          [ 0.4751,  0.4065, -0.2591, -0.5601, -0.2064],\n",
      "          [ 0.3615,  0.3217, -0.0727, -0.2813, -0.2295]]],\n",
      "\n",
      "\n",
      "        [[[-0.2922, -0.2412,  0.1118,  0.1287, -0.4519],\n",
      "          [-0.6464, -0.0107,  0.4840,  0.5794,  0.1810],\n",
      "          [-0.4460, -0.1608,  0.2682,  0.9490,  0.4306],\n",
      "          [-0.1609,  0.1633,  0.5715,  0.6126,  0.0575],\n",
      "          [ 0.1954,  0.4175,  0.1950,  0.0760, -0.1388]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1972,  0.0756,  0.0230, -0.0901, -0.0092],\n",
      "          [ 0.2219,  0.6511,  0.6463,  0.3962,  0.2364],\n",
      "          [-0.2721,  0.3888,  0.5010,  0.4422,  0.5578],\n",
      "          [-0.5137, -0.4386, -0.1499,  0.0387,  0.4373],\n",
      "          [-0.7671, -0.8717, -0.7431, -0.6631, -0.1831]]]])), ('conv1.bias', tensor([-0.0297, -0.1321, -0.0220])), ('conv2.weight', tensor([[[[-0.0297,  0.1730, -0.3932],\n",
      "          [-0.1410, -0.0406,  0.1142],\n",
      "          [-0.1711, -0.4478,  0.1224]],\n",
      "\n",
      "         [[ 0.2099, -0.2872, -0.4535],\n",
      "          [ 0.0464,  0.0863, -0.1034],\n",
      "          [ 0.3313,  0.1037,  0.1029]],\n",
      "\n",
      "         [[-0.3394, -0.6062, -0.0117],\n",
      "          [ 0.3960,  0.1183, -0.2221],\n",
      "          [ 0.1648,  0.3801,  0.1256]]],\n",
      "\n",
      "\n",
      "        [[[-0.9424, -0.5140, -0.3697],\n",
      "          [-0.0084, -0.4807, -0.1102],\n",
      "          [ 0.3532, -0.0035,  0.0197]],\n",
      "\n",
      "         [[-0.2026, -0.3189, -0.2538],\n",
      "          [ 0.4079,  0.0482,  0.1011],\n",
      "          [ 0.0999,  0.0038,  0.2174]],\n",
      "\n",
      "         [[-0.6652, -0.9219, -0.9619],\n",
      "          [ 0.0747,  0.0680,  0.3173],\n",
      "          [ 0.4630,  0.1250,  0.1783]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0476, -0.2633, -0.4358],\n",
      "          [ 0.0074, -0.6163, -0.0158],\n",
      "          [ 0.1544, -0.2371,  0.6340]],\n",
      "\n",
      "         [[-0.6481, -0.1027,  0.4766],\n",
      "          [-0.4299,  0.3470,  0.2642],\n",
      "          [-0.4078,  0.1934,  0.2710]],\n",
      "\n",
      "         [[-0.3550, -0.6125, -0.5079],\n",
      "          [-0.4450, -0.0452, -0.6587],\n",
      "          [-0.3244, -0.0470, -0.9271]]],\n",
      "\n",
      "\n",
      "        [[[-0.0409,  0.0335,  0.0344],\n",
      "          [ 0.0804,  0.0278,  0.2751],\n",
      "          [-0.0298,  0.0511, -0.2560]],\n",
      "\n",
      "         [[ 0.1998,  0.2329,  0.5163],\n",
      "          [-0.1342,  0.1214, -0.1698],\n",
      "          [-0.3580, -0.3372, -0.7100]],\n",
      "\n",
      "         [[-0.0181, -0.2656, -0.1339],\n",
      "          [ 0.4062,  0.5094,  0.0612],\n",
      "          [ 0.2012, -0.1517, -0.1770]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0140,  0.3930,  0.2576],\n",
      "          [ 0.2994,  0.2980,  0.1219],\n",
      "          [ 0.1297, -0.1756, -0.1664]],\n",
      "\n",
      "         [[ 0.3938,  0.0500,  0.0395],\n",
      "          [ 0.2706, -0.2293, -0.3112],\n",
      "          [ 0.0862, -0.0957, -0.2429]],\n",
      "\n",
      "         [[-0.1885, -0.1495, -0.1890],\n",
      "          [-0.6082, -0.3910, -0.1127],\n",
      "          [-0.5550, -0.3363,  0.0372]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6667,  0.3302, -0.4423],\n",
      "          [ 0.3179,  0.1365, -0.0660],\n",
      "          [-0.2223, -0.4339,  0.1466]],\n",
      "\n",
      "         [[ 0.3051, -0.6490, -0.0146],\n",
      "          [-0.4831, -0.1166,  0.2819],\n",
      "          [-0.3774,  0.0327,  0.2144]],\n",
      "\n",
      "         [[ 0.0095, -0.5271, -0.3113],\n",
      "          [-0.0914,  0.0017, -0.1587],\n",
      "          [-0.5440, -0.1847,  0.1091]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1254, -0.1774, -0.3128],\n",
      "          [ 0.4195, -0.0814, -0.4260],\n",
      "          [ 0.4593,  0.2323, -0.1803]],\n",
      "\n",
      "         [[-0.4090, -0.5007,  0.3028],\n",
      "          [-0.2454, -0.6616,  0.4138],\n",
      "          [ 0.1381, -0.5887, -0.0999]],\n",
      "\n",
      "         [[-0.0332, -0.0594,  0.1191],\n",
      "          [-0.7594, -0.1289,  0.3637],\n",
      "          [-0.5638, -0.3956,  0.5507]]],\n",
      "\n",
      "\n",
      "        [[[-0.3525,  0.0074,  0.7697],\n",
      "          [-0.5947, -0.9082,  0.3234],\n",
      "          [-1.0389, -0.3668,  0.1247]],\n",
      "\n",
      "         [[ 0.3251,  0.1485, -0.3976],\n",
      "          [-0.1701,  0.3985, -0.4131],\n",
      "          [-0.3679,  0.3485,  0.0288]],\n",
      "\n",
      "         [[ 0.1737, -0.1340, -0.8981],\n",
      "          [ 0.2307, -0.0085, -0.3425],\n",
      "          [-0.1598, -0.0485,  0.3196]]],\n",
      "\n",
      "\n",
      "        [[[-0.0975,  0.0834, -0.2002],\n",
      "          [-0.6149, -0.2360, -0.4502],\n",
      "          [-0.6866, -0.4023, -0.2640]],\n",
      "\n",
      "         [[ 0.1021, -0.1729, -0.1578],\n",
      "          [-0.0600, -0.1080, -0.2408],\n",
      "          [-0.2178, -0.1089, -0.3013]],\n",
      "\n",
      "         [[ 0.3447,  0.1466, -0.3070],\n",
      "          [ 0.3148,  0.4913,  0.7486],\n",
      "          [-0.1369, -0.1796,  0.4147]]]])), ('conv2.bias', tensor([ 5.3017e-04,  1.5196e-03, -4.4676e-03,  6.7983e-03, -3.2050e-01,\n",
      "        -1.6708e-01,  2.6162e-02, -7.5420e-01, -2.1986e-02])), ('fc3.weight', tensor([[-0.0568,  0.0444,  0.1507,  ...,  0.1089,  0.0709, -0.2473],\n",
      "        [-0.0813, -0.0743, -0.0728,  ..., -0.1183,  0.1010,  0.2016],\n",
      "        [-0.2305, -0.0985,  0.0822,  ...,  0.0652,  0.2259,  0.4212],\n",
      "        ...,\n",
      "        [ 0.3718,  0.1373,  0.0856,  ..., -0.1541, -0.2240, -0.0566],\n",
      "        [-0.0557, -0.1135, -0.1411,  ...,  0.0092,  0.1004, -0.1668],\n",
      "        [-0.0895, -0.0393,  0.1341,  ..., -0.0389, -0.2271, -0.0246]])), ('fc3.bias', tensor([-0.0207, -0.0829,  0.1683,  0.0713, -0.1228,  0.0062, -0.0178, -0.0368,\n",
      "         0.1603, -0.0215]))])\n",
      "epoch : 4/4 | train loss : 0.1494\n",
      "epoch : 4/4 | train loss : 0.1083\n",
      "epoch : 4/4 | train loss : 0.1049\n",
      "epoch : 4/4 | train loss : 0.1161\n",
      "epoch : 4/4 | train loss : 0.1201\n",
      "epoch : 4/4 | train loss : 0.0984\n",
      "epoch : 4/4 | train loss : 0.1040\n",
      "epoch : 4/4 | train loss : 0.1163\n",
      "epoch : 4/4 | train loss : 0.0989\n",
      "epoch : 4/4 | train loss : 0.0812\n",
      "epoch : 4/4 | train loss : 0.0996\n",
      "epoch : 4/4 | train loss : 0.1196\n",
      "epoch : 4/4 | train loss : 0.1004\n",
      "epoch : 4/4 | train loss : 0.1489\n",
      "epoch : 4/4 | train loss : 0.0771\n",
      "epoch : 4/4 | train loss : 0.1007\n",
      "epoch : 4/4 | train loss : 0.1011\n",
      "epoch : 4/4 | train loss : 0.1155\n",
      "epoch : 4/4 | train loss : 0.1189\n",
      "epoch : 4/4 | train loss : 0.1072\n",
      "epoch : 4/4 | train loss : 0.0991\n",
      "epoch : 4/4 | train loss : 0.1304\n",
      "epoch : 4/4 | train loss : 0.1011\n",
      "epoch : 4/4 | train loss : 0.1173\n",
      "epoch : 4/4 | train loss : 0.1204\n",
      "epoch : 4/4 | train loss : 0.0877\n",
      "epoch : 4/4 | train loss : 0.1041\n",
      "epoch : 4/4 | train loss : 0.0844\n",
      "epoch : 4/4 | train loss : 0.0731\n",
      "epoch : 4/4 | train loss : 0.0841\n",
      "OrderedDict([('conv1.weight', tensor([[[[ 0.2225,  0.2048,  0.3410,  0.0539, -0.0700],\n",
      "          [ 0.1879,  0.2232,  0.1843, -0.1048, -0.3306],\n",
      "          [ 0.4577,  0.5259,  0.0119, -0.6158, -0.2492],\n",
      "          [ 0.4568,  0.4474, -0.2653, -0.5723, -0.2048],\n",
      "          [ 0.3699,  0.3378, -0.1047, -0.3166, -0.2242]]],\n",
      "\n",
      "\n",
      "        [[[-0.2889, -0.2396,  0.0708,  0.1211, -0.5195],\n",
      "          [-0.7373,  0.0072,  0.4650,  0.5661,  0.1148],\n",
      "          [-0.5565, -0.2112,  0.2683,  0.9818,  0.4771],\n",
      "          [-0.1972,  0.1618,  0.6021,  0.5888,  0.0307],\n",
      "          [ 0.2395,  0.4270,  0.1120,  0.0249, -0.1278]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2011,  0.1038,  0.0312, -0.1182, -0.0357],\n",
      "          [ 0.2200,  0.7146,  0.6772,  0.4089,  0.1987],\n",
      "          [-0.3453,  0.3846,  0.5483,  0.4596,  0.5058],\n",
      "          [-0.5744, -0.4287, -0.1329,  0.0511,  0.4324],\n",
      "          [-0.7578, -0.8851, -0.7545, -0.6596, -0.1550]]]])), ('conv1.bias', tensor([-0.0452, -0.1391, -0.0352])), ('conv2.weight', tensor([[[[ 0.0088,  0.2194, -0.4360],\n",
      "          [-0.1385, -0.0772,  0.1443],\n",
      "          [-0.1540, -0.5187,  0.1617]],\n",
      "\n",
      "         [[ 0.1404, -0.2717, -0.4455],\n",
      "          [ 0.0246,  0.0213, -0.1312],\n",
      "          [ 0.3360,  0.1468,  0.0787]],\n",
      "\n",
      "         [[-0.4552, -0.6760,  0.0197],\n",
      "          [ 0.4899,  0.0648, -0.2830],\n",
      "          [ 0.1610,  0.3996,  0.1156]]],\n",
      "\n",
      "\n",
      "        [[[-0.9930, -0.5490, -0.3570],\n",
      "          [ 0.0237, -0.5157, -0.1297],\n",
      "          [ 0.3902, -0.0671, -0.0200]],\n",
      "\n",
      "         [[-0.2037, -0.3730, -0.3220],\n",
      "          [ 0.4670,  0.1097,  0.0685],\n",
      "          [ 0.1321,  0.0277,  0.2517]],\n",
      "\n",
      "         [[-0.6737, -0.9835, -0.9927],\n",
      "          [ 0.0788,  0.1098,  0.3308],\n",
      "          [ 0.4584,  0.1030,  0.1460]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0411, -0.3318, -0.3957],\n",
      "          [ 0.0276, -0.5597, -0.0557],\n",
      "          [ 0.1462, -0.2463,  0.6279]],\n",
      "\n",
      "         [[-0.6875, -0.1293,  0.4465],\n",
      "          [-0.4472,  0.3561,  0.2806],\n",
      "          [-0.4468,  0.1991,  0.2720]],\n",
      "\n",
      "         [[-0.3304, -0.6318, -0.5490],\n",
      "          [-0.4632, -0.0417, -0.7145],\n",
      "          [-0.3081, -0.0046, -0.9696]]],\n",
      "\n",
      "\n",
      "        [[[-0.0273, -0.0466,  0.0343],\n",
      "          [ 0.0334,  0.1039,  0.2671],\n",
      "          [-0.0884,  0.0966, -0.2585]],\n",
      "\n",
      "         [[ 0.1479,  0.2786,  0.5600],\n",
      "          [-0.1427,  0.1399, -0.1531],\n",
      "          [-0.3696, -0.4041, -0.7406]],\n",
      "\n",
      "         [[-0.0282, -0.2624, -0.1883],\n",
      "          [ 0.3856,  0.5312,  0.0666],\n",
      "          [ 0.2917, -0.1866, -0.2339]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0339,  0.4414,  0.2588],\n",
      "          [ 0.3494,  0.2971,  0.1152],\n",
      "          [ 0.1102, -0.2131, -0.1774]],\n",
      "\n",
      "         [[ 0.4216,  0.0230, -0.0055],\n",
      "          [ 0.3079, -0.2742, -0.3369],\n",
      "          [ 0.0802, -0.1034, -0.2800]],\n",
      "\n",
      "         [[-0.2167, -0.0954, -0.1885],\n",
      "          [-0.6587, -0.3954, -0.1359],\n",
      "          [-0.5706, -0.3172,  0.1492]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6774,  0.3357, -0.5229],\n",
      "          [ 0.3204,  0.0826, -0.0885],\n",
      "          [-0.2378, -0.4405,  0.1282]],\n",
      "\n",
      "         [[ 0.3281, -0.6286, -0.0262],\n",
      "          [-0.4591, -0.1138,  0.3012],\n",
      "          [-0.3682,  0.0387,  0.2000]],\n",
      "\n",
      "         [[ 0.0640, -0.5691, -0.4492],\n",
      "          [-0.0567, -0.0305, -0.1362],\n",
      "          [-0.5613, -0.1711,  0.1681]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1067, -0.1990, -0.3179],\n",
      "          [ 0.4510, -0.0843, -0.3620],\n",
      "          [ 0.5222,  0.2907, -0.1610]],\n",
      "\n",
      "         [[-0.4322, -0.4809,  0.3459],\n",
      "          [-0.2979, -0.7013,  0.4189],\n",
      "          [ 0.1376, -0.6675, -0.0943]],\n",
      "\n",
      "         [[ 0.0276, -0.0591,  0.0942],\n",
      "          [-0.7954, -0.1828,  0.3373],\n",
      "          [-0.5786, -0.4679,  0.5589]]],\n",
      "\n",
      "\n",
      "        [[[-0.3463,  0.0297,  0.8060],\n",
      "          [-0.6529, -0.9025,  0.3279],\n",
      "          [-1.1206, -0.3437,  0.1317]],\n",
      "\n",
      "         [[ 0.3075,  0.1970, -0.4064],\n",
      "          [-0.1927,  0.4516, -0.4878],\n",
      "          [-0.4055,  0.4141,  0.0089]],\n",
      "\n",
      "         [[ 0.1058, -0.1574, -0.8170],\n",
      "          [ 0.2001, -0.0232, -0.3475],\n",
      "          [-0.1511, -0.0654,  0.3274]]],\n",
      "\n",
      "\n",
      "        [[[-0.1057,  0.0917, -0.1976],\n",
      "          [-0.6610, -0.2634, -0.5245],\n",
      "          [-0.7242, -0.3842, -0.2402]],\n",
      "\n",
      "         [[ 0.0860, -0.1424, -0.1470],\n",
      "          [-0.0551, -0.0613, -0.2617],\n",
      "          [-0.2107, -0.1277, -0.3617]],\n",
      "\n",
      "         [[ 0.3602,  0.1414, -0.2926],\n",
      "          [ 0.3472,  0.5056,  0.7809],\n",
      "          [-0.1618, -0.2101,  0.3958]]]])), ('conv2.bias', tensor([-0.0009,  0.0071,  0.0696,  0.0118, -0.3395, -0.1573,  0.0228, -0.8080,\n",
      "        -0.0335])), ('fc3.weight', tensor([[-0.0432,  0.0451,  0.1443,  ...,  0.1350,  0.1107, -0.2702],\n",
      "        [-0.0531, -0.0616, -0.0799,  ..., -0.1319,  0.1042,  0.2247],\n",
      "        [-0.2503, -0.0927,  0.0957,  ...,  0.0336,  0.1935,  0.4726],\n",
      "        ...,\n",
      "        [ 0.3911,  0.1436,  0.0978,  ..., -0.2048, -0.1982, -0.0359],\n",
      "        [-0.0989, -0.1228, -0.1492,  ...,  0.0028,  0.1040, -0.2139],\n",
      "        [-0.0788, -0.0534,  0.1244,  ..., -0.0353, -0.2600,  0.0143]])), ('fc3.bias', tensor([-0.0124, -0.0939,  0.1955,  0.0701, -0.1537,  0.0064, -0.0234, -0.0459,\n",
      "         0.2089, -0.0475]))])\n"
     ]
    }
   ],
   "source": [
    "small_model = SmallModel()\n",
    "\n",
    "optimizer_pt = optim.Adam(small_model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8, amsgrad=True)\n",
    "\n",
    "train_losses_pt, test_acc_pt = train(small_model, optimizer_pt, criterion, nb_epochs, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a205f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
